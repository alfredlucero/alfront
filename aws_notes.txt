AWS Certified Developer Associate Notes (2016):

10000 Feet Overview
- Messaging, Security and Idenitty, Management Tools, Storage, Databases, Networking and Content Delivery, Compute,
AWS Global Infrastructure

- AWS Global Infrastructure - Involves regions and availability zones around the world
-> Region - geographical area, an independent collection of AWS coputing resources in a defined geography, around 11 (2016)
-> each region consists of 2 or more availability zones
-> availability zone (AZ) = a data center, distinct locations from within an AWS region that are engineered to be isolated
from failures
-> Edge location: CDN (content delivery network) endpoints for CloudFront (allows you to cache large media objects in cloud),
more edge locations than regions, points of presence for CloudFront

- Networking and Content Delivery
-> Virtual Private Cloud (VPC): virtual data centers in all the regions around the world where you will deploy your assets
-> Route53: Amazon's DNS services (IP address to domain translation), domain registration, 53 is DNS port
(route 66 first interstate highway across US)
-> CloudFront: Amazon's CDN, edge locations that will cache your assets like large media files
-> Direct Connect: Connecting up your office or physical data centers to AWS using dedicated telephone line for security or
need a reliable internet connection to push a lot of data

- Compute
-> Elastic Computing 2 (EC2): virtual machines in the cloud, can log in through SSH, can install things with the OS
-> EC2 Container Service (ECS): supports Docker containers and running applications on managed cluster of EC2 instances, 
no need to scale and operate your own cluster
-> Elastic Beanstalk: can upload your code application and will provision the underlying infrastructure for your app
-> Lambda: serverless, upload your code and it will respond to events
-> Lightsail: out of the box cloud to deploy say a wordpress or drupal site

- Storage
-> S3 (Simple Storage Service: virtual disk in the cloud where you can store objects such as files, word documents, 
movies, etc.; not to install an
app/game or database (needs block-based storage) i.e. Dropbox uses this
-> Glacier: archive your files from S3 off, low cost, store files for compliance reasons, can't access them immediately
(takes 3-4 hours)
-> EFS (Elastic File Service): file-based storage you can share, can install applications and databases and share it with
multiple applications
-> Storage Gateway: connecing S3 to on premise headquarter, virtual machine image
-> EBS: virtual disk attached to EC2 instance for block-based storage

- Databases
-> RDS (relational database service): includes Postgres, MySQL, Maria, Oracle, Aurora
-> DynamoDB: noSQL database, scalable, high performance
-> Redshift: warehousing service, transfer copy of production database into redshift and run queries on it
-> Elasticache: caching data in cloud to take load off database and return data much quicker

- Migration
-> Snowball: import/export of disks, to move terabytes of data into the cloud
-> DMS (Database Migration Services): migrate on premise database onto AWS cloud or into other regions/AWS services,
uses replication and can convert databases, cheaper than Oracle services
-> SMS (Server Migration Services): targets VMware virtual machines to migrate and replicate them onto AWS cloud,
can do around 50 concurrently at the same time

- Analytics
-> Athena: run SQL queries on S3 buckets like if you want to look at the csv files/objects
-> EMR (Elastic Map Reduce): big data processing
-> Cloud Search: fully managed service for search capabilities in app/website
-> Elastic Search: open source framework service for search capabilities in app/website
-> Kinesis: streaming and analyzing real-time data in massive scale like for financial transactions, social media streams, etc
-> Data Pipeline: lets you move data from one place to another like from S3 into DynamoDB
-> Quick Sight: rich dashboards for data from applications like S3, DynamoDB, etc.

- Security and Identity
-> IAM (Identity Access Management): how you authenticate to AWS, assign user permissions/groups, etc.
-> Inspector: agent installed on VM that inspects those VMs and does security reporting
-> Certifcate Manager: free SSL certificates for domain names
-> Directory Service: connecting up active directory to AWS
-> WAF (Web Application Firewall): application level protection to your website, stop SQL injections/XSS
-> Artifacts: documentation in AWS console, look at compliance docs/certification

- Management Tools:
-> Cloud Watch: monitors performance of your AWS environment like EC2 disk/RAM utilization
-> Cloud Formation: turning your infrastructure into code, can provision production environments into templates and deploy at
will with single command line
-> Cloud Trail: auditing AWS resources and changes to environment
-> Opsworks: automating deployments using chef
-> Config: automatically monitors environment and gives warnings if breaks configurations, set alerts 
-> Service Catalog: designed for larger enterprises with specific images/AWS services to authorize, build out what you 
authorize in the organization
-> Trusted Advisor: gives you tips on how to customize, security fixes, scans environment

- Application Services
-> Step Functions: visualizing what's going on inside applications and which microservices it is using
-> SWF (Simple Work Flow): coordinating both automated and human-led tasks
-> API Gateway: door to allow you to create and secure API that accesses backend data/services
-> AppStream: streaming Desktop applications to your users
-> Elastic Transcoder: changes video format to suit all different devices

- Developer Tools
-> CodeCommit: like Github, place to store your code in the cloud, open/closed
-> CodeBuild: way of compiling your code in different environments, pay by the minute of code build
-> CodeDeploy: way of deploying code to EC2 instances in automated and regulated way
-> CodePipeline: keeps track of different versions of code

- Mobile Services
-> Mobile Hub: console for mobile apps
-> Cognito: easy for users to sign in and sign up into apps
-> Device Farm: improve quality of apps by testing on lots of different devices
-> Mobile Analytics: collect and analyze app usage data
-> Pinpoint: understand and engage with application users, like Google Analytics

- Business Productivity
-> WorkDocs: securely storing work documents
-> WorkMail: exchange for AWS

- Internet of Things
-> iOT: keeping track of iOT devices

- Desktop and App Streaming
-> WorkSpaces: like VDI, having desktop in cloud
-> AppStream 2.0: streaming desktop applications to users

- Artificial Intelligence
-> alexa: voice service in the cloud, communicate using the Echo, talking to Lambda, includes Lex inside (don't need Echo)
-> Polly: helps with rendering voices and different languages
-> Machine Learning: give dataset and tell the outcomes, predict future outcomes based on analysis of training set
-> Rekognition: upload a picture and will give you tags about what is in it, facial recognition

- Messaging
-> SNS (Simple Notification Service): notify via email/text/publish to HTTP endpoints
-> SQS (Simple Queue Service): decoupling your applications, queue system to post jobs to a queue, EC2 instance pulls
SQS jobs and acts upon them
-> SES (Simple Email Service): sending and receiving emails using AWS


Identity Access Management (IAM)
- allows you to manage users and their level of access to the AWS console, administrating a company's AWS account
- centralized control and shared access to your AWS account, granular permissions, identity federation 
(including Active Directory, Facebook, Linkedin, etc.)
- multifactor authentication, provide temporary access for users/devices and services where necessary
- password rotation policy, integrates with other services
- supports PCI DSS Compliance
- groups -> collection of users under one set of permissions; users -> end users; roles -> assign them to AWS resources;
policies -> documents that defines one or more permissions
- doesn't require region selection (global)
- can change users sign-in link (setting account alias) -> https://randomlink-a.signin.aws.amazon.com/console

- activating MFA on root account (email address you used to sign up to aws) -> create users and group them with permissions
on the root account and only login in when necessary
-> programmatic access/AWS Management Console access -> group policies such as sysadmin, admin, etc.
-> access key ID and secret access key given out allows you to programmatically interact with AWS services
-> settings roles to allow AWS services to talk to other services like EC2 to S3

- Security Token Service (STS): grants users limited and temporary access to AWS resources
-> comes from federation (typically Active Directory)
-> federation with mobile apps (OpenID providers to log in)
-> cross account access (one AWS account access resources in another)
-> federation: combining or joining a list of users in one domain such as IAM with a list of users in another domain such
as Active Directory, Facebook, etc.
-> identity broker: service that allows you to take an identity from point A and join it/federate it to point B
-> identity store - services like Active Directory, Facebook, Google, etc.
-> identities - user of a service like Facebook, etc.
-> STS would return an access key, secret access key, token and duration (token's lifetime) if validated by broker and IAM
-> 1. Develop an Identity Broker to communicate with LDAP and AWS STS
-> 2. Identity Broker always authenticates with LDAP first, then AWS STS 
-> 3. Application then gets temporary access to AWS resources
-> (roles) OR can get an IAM role to associate with user and then auth with STS and assumes IAM role to interact with S3

- Active Directory Federation
-> can authenticate with this using SAML (Security Assertion Markup Language), AssumeRoleWithSAML
-> AWS sign-in endpoint for SAML is https://signin.aws.amazon.com/saml
-> authenticate with active directory first then assigned to temporary security credential

- Web Identity Federation with Mobile Applications
-> can authenticate your application using Facebook, Google, Amazon account, etc.
-> web identity federation playground to sign in -> given access token that expires and then obtain temporary security 
credentials with AssumeRoleWithWebIdentity

- consists of users, groups (group users and apply policies to them collectively), roles, policy documents (JSON),
universal and does not apply to specific regions, "root account" is account created when first set up your AWS account
that has complete Admin access, new users have no permissions when first created, new users assigned access key ID and
secret access keys when first created (can't be used to log in to the console but can access AWS via APIs and CL),
only get to view those once and if you lose them you have to regenerate them so must save in secure location,
setup MFA on root account, password rotation policies


EC2 (Elastic Compute Cloud)
- web service that provides resizable compute capacity in the cloud, quickly scale up and down
- only pay for capacity that you actually use
- On Demand: pay fixed rate by the hour or by the second (with Linux) with no commitment
-> low cost and flexibility, short term/spiky workloads in apps
- Reserved: capacity reservation and offer significant discount on the hourly change for an instance (1year or 3years)
-> steady state or predictable usage, reserved capacity, upfront payments to reduce total computing costs
-> steady RIs, convertible RIs, scheduled RIs
- Spot: bid whatever price you want for instance capacity to provide greater savings if your app have flexible start and end times
-> only feasible at very low compute prices, urgent computing needs
-> if you terminate the instance, you pay for the hour
-> if AWS terminates the spot instance, you get the hour it was terminated in for free
- Dedicated Hosts: physical EC2 servers dedicated for your use, reduce costs by using your existing server-bound software licenses
-> regulatory requirements that may not support multi-tenant virtualization, licensing, purchased on-demand by the hourly/reservation
- instance types: D2 (density), R4 (memory optimized, like RAM), M4 (main choice, general purpose/app servers), C4, T2(lowest cost, general purpose, web servers, small dbs), etc.
-> mnemonic: DRMCGIFTPX - D for density, R for RAM, M for main choice for general purpose apps, C for compute, G for graphics
-> I for IOPS, F for FPGA, T for cheap general purpose like T2 Micro, P for graphics/pics, X for extreme memory

EBS
- allows you to create storage volumes and attach them to Amazon EC2 instances; once attached you can create a file system
on top of these volumes, run a database, or use them in any other way you would use a block devices
- placed in specific availability zone where they are automatically replicated to protect you from the failure of a single component
- General Purpose SSD (GP2)
-> balances both price and performance, ratio of 3 IOPs per GB with up to 10,000 IOPS
- Provisioned IOPS SSD (IO1)
-> IO intensive apps such as large relational or NoSQL databases, use if need more than 10,000 IOPS
- Throughput Optimized HDD (ST1)
-> big data, data warehouses, log processing, cannot be a boot volume, frequently accessed workloads
- Cold HDD (SC1)
-> lowest cost storage for infrequently accessed workloads, file server, cannot be boot volume, less frequently accessed data
- Magnetic (Standard)
-> lowest cost per gigabyte of all EBS volume types that is bootable, ideal when data accessed infrequently and the lowest storage cost important
- cannot mount 1 EBS volume to multiple EC2 instances, instead use EFS

Setting up EC2 Instance
- Click Launch Instance -> Choose Amazon Machine Image (AMI - virtual machines to boot up) i.e. Amazon Linux with AWS CL Tools
-> Choose Instance Type like free t2 micro (m type is main one for applications) -> Configure instance details (one subnet = 1 availability zone)
-> Add storage (root volume - virtual hard disk on cloud to boot from like general purpose SSD, etc; EBS volumes possible, default delete on termination, can encrypt volumes)
-> Add tags (key value pairs like name, department, team, etc.) -> Security group (virtual firewalls and rules, what traffic you want to allow through like SSH, HTTP/HTTPS to view website,
custom IP address range to access - 0.0.0.0/0 is anyone) -> Review Instance Launch (can set public/private key pair to connect to instance securely and then view instances)
- Check public IP address description of instance to SSH into it (go to private key download - .pem file)
-> i.e. ssh ec2user@<publicIpAddress> -i some_private_key.pem; sudo yum update -y
-> going to public DNS address will open up your webserver if you have one
- Security group: 1 instance can have multiple, virtual firewall, rules to handle types of inbound/outbound traffic like HTTP/HTTPS/SSH and IP ranges
-> anything allowed in is allowed out (stateful)
-> all inbound traffic blocked by default, all outbound traffic is allowed by default, changes to security group take effect immediately
-> can have any number of ec2 instances within a security group, can have multiple security groups attached to EC2 instances
-> cannot block specific IP addreses using security groups, instead use Network Access Control Lists
-> can specify allow rules, not deny rules
- Upgrading EBS volumes: can be changed on the fly (except for magnetic standard), best practice to stop ec2 instance and then change volume
-> can change volume types by taking snapshot and using snapshot to create a new volume, if you change volume on fly, must wait 6 hours before making another change
-> can scale EBS volumes up only, volumes must be in the same AZ as EC2 instances
- EFS (Elastic File System): file storage service fo EC2 instances, elastic storage capacity, growing and shrinking automaticcaly as one adds and removes files
-> pay for storage you use (no pre-provisioning required), supports Network File System version 4 (NFSv4) protocol
-> can scale up to petabytes, support thousands of concurrent NFS connections, data is stored across multiple AZs within a region
-> read after write consistency
- CLI commands: AWS EC2 DESCRIBE-INSTANCES (current running instances)
-> AWS EC2 DESCRIBE-IMAGES (all images available to you)
-> AWS EC2 RUN-INSTANCES (launch/create new instances) vs. START-INSTANCES (start a stopped instance) vs. terminate
- can run a bash script when starting up EC2 instance
- Elastic Load Balancers: instanced monitored by ELB are reported as InService or OutOfService
-> Health Checks check the instance health by talking to it
-> have own DNS name, never given an IP address, read ELB FAQ for Classic Load Balancers
- SDKs for like Android, iOS, JS (browser), etc., default region = US-EAST-1
- Lambda: compute service where you can upload your code and create a Lambda Functions
-> takes care of provisioning and managing servers to run code
-> event-driven, runs code in response to events like changes to data in S3 bucket or DynamoDB table
-> run code in response to HTTP requests using Amazon API Gateway or API calls made using AWS SDKs
-> need to select blueprint, configure valid triggers through things like API Gateway (autoscales)
-> two users invoking two HTTP requests invokes two Lambda functions, one set of code responding to multiple requests
-> 1 million free invocations per month, supports Node.js, Java, C#, Python
-> priced based on number of requests, $0.20 per 1 million requests thereafter the first 1 million requests free - low costs
-> also billed on duration: calculated from time your code begins executing until it returns or otherwise terminates rounded up to nearest 100ms,
-> depends on amount of memory you allocate to your function, cannot execute over 5 minutes
-> benefits: no servers, continuous scaling, cheap i.e. everytime speaking to amazon echo, responds with Lambda functions
-> scales out (not up) automatically, functions are independent (1 event = 1 function), serverless, know what services are serverless such as S3, API Gateway, DynamoDB
-> can trigger other lambda functions, 1 event can lead to x functions if functions trigger other functions
-> AWS X-ray allows you to debug what is happening, can do things globally like back up S3 buckets, etc.

S3 Essentials
- Simple Storage Service, highly-scalable object storage, place to store files in cloud
- object-based storage and data is spread across multiple devices and facilities i.e. used by Dropbox
- files can be from 0 bytes to 5TB, unlimited storage, files stored in buckets (folder)
- S3 is universal namespace, names must be unique globally, creates DNS address like https://s3-region.amazonaws.com/bucketname
- when you upload a file to S3, will receive an HTTP 200 code if successful
- data consistency model: read after write consistency for PUTS of new objects (can read immediately after creating new object)
-> eventual consistency for overwrite PUTS and DELETES (can take some time for updates/deletes to propagate)
-> will either get the old version or new version, not a partial corrupted one
-> simple key (name of object) value (data) store, version ID, metadata (like date uploaded, last updated), 
subresources (exist underneath object) like access control lists and torrent
- 99.99% availability SLA, 11 x 9's durability for information, tiered storage available
- lifecycle management, versioning of objects, encryption, secure data using access control lists and bucket policies
- storage tiers/classes
-> S3: stored redundantly across multiple devices in multiple facilities and is designed to sustain loss of 2 facilities concurrently
-> S3-IA (infrequently accessed): for data accessed less frequently but requires rapid access when needed, lower fee than S3
but charged a retrieval fee
-> Reduced Redundancy Storage: designed to provide 99.99% durability and availability of objets over a given year
-> Glacier: very cheap but archival only, 3-5 hours to restore from glacier, as little as $0.01 per gb per month, great for infrequently accessed data
- charged for storage, requests, storage management pricing (tagging and tracking costs), data transfer pricing (coming in is free, replication in another region is charged),
transfer acceleration (transfers of files over long distances, takes advantage of CloudFront's globally distributed edge locations, data arrives at edge and routed to S3 over optimized network path)
- not suitable to install an operating system
- creating an S3 bucket in console: name and region -> properties (version, logging, tagging) -> permissions (public/system, users)
-> clicking on files in bucket so you can see the links to objects but they are private by default, need to set objects to public
-> per object can set storage class, encryption option, metadata, tags (individual objects don't inherit the bucket tags)
-> per bucket can set versioning, logging, static website hosting, tags, transfer acceleration, events, requester pays
-> bucket policies or access control lists, can set CORS configuration, lifecycle/replication/analytics/metrics/inventory management and reports
-> client and server side encryption (with S3 managed keys SSE-S3, KMS, Customer Provided Keys), control access to buckets with bucket ACL or bucket policies
-> by default buckets are private and all objects stored inside them are private
- creating an S3 website: bucket name has to be exact name as domain based on route 53
-> can enable static website hosting and use the bucket to host a website, set index and error documents, redirects
-> need to grant public read access so you can access it online
- Cross Origin Resource Sharing (CORS) in S3
-> allowing JavaScript code in one resource/bucket to access those in another bucket/resource like bucketname.com/<resource>
-> go to CORS configuration editor, <AllowedOrigin ...resource/bucket url...>
- build a serverless webpage with API Gateway and Lambda
-> i.e. flow: user will make a GET request to resolve domain name through Route 53, go to S3 with index.html/error.html, hitting button will trigger GET request through
API Gateway and then trigger event to Lambda function to print out your name and response passed back all the way to user
- using Amazon Polly (text to speech converter): type text and download speech as MP3 and store in S3+DynamoDB and use Lambda functions
-> first create S3 bucket to host static website, another to store Polly audio MP3 files, use Simple Notification Service to dispatch new topic notifications, and Lambda functions
to hit SNS and DynamoDB (need to set up role with JSON policies to allow Lambda to affect DynamoDB, S3, etc.)
-> Lambda function will update DynamoDB record and send notification about new post to SNS to trigger lambda to convert and save MP3 to S3, can use environment variables for table name/SNS topic ARN (name), test events
-> need to set up GET and POST methods in API Gateway with Lambda function integration type and CORS enabled
- building an Alexa skill (Echo/FireTv/RaspberryPi is hardware, voice service is Alexa): automatic speech recognition and natural language understanding
-> skill service: AWS Lambda; skill interface: invocation name, intent schema, slot type, utterances in console
-> intent is what you want to do (like AMAZON.HelpIntent) in json file, utterances are ways of saying a command in .txt file
-> Alexa Skill Kit in Lambda, can test in console with text as well, return facts mp3 files
- S3 Version Control:
-> once versioning turned on, can only be disabled not removed; can track versions of objects in bucket (sum up all versions' sizes for cost); can restore versions deleted, backup tool
-> integrates with lifecycle rules, MFA (multi-factor authentication) delete capability
- Cross Region Replication:
-> can replicate your buckets across different regions, only new objects replicated over not existing ones unless using CLI
-> can use awscli (pip install awscli), aws configure (pass in access key ID and secret access key)
-> aws s3 cp --recursive s3://<frombucketname> s3://<tobucketname>
-> versioning must be enabled on both source and destination, regions must be unique, files in existing bucket not replicated automatically but subsequent ones are replicated automatically
-> cannot replicate to multiple buckets or use daisy chaining, delete markers are replicated, deleting individual versions or delete markers will not be replicated
- S3 Lifecycle management, S3-IA and Glacier
-> Glacier for data archival low-cost storage service
-> lifecycle management to expire objects, automate transition to tiered storage, create rules to manage objects
-> i.e. transition to Standard-IA 30 days after object creation, Glacier after 60 days for current version or previous versions; configure expiration/permanent deletion after X days
-> can be used in conjunction with versioning, applied to current and previous versions, transition to standard-IA with 128Kb and 30 days after creation date,
archive to glacier 30 days after IA, can permanently delete
- CloudFront
-> content delivery network (CDN): system of distributed servers (network) that deliver webpages and other web content to a user based on the geographic locations of the user, the origin of the webpage and content delivery server
-> edge location: location where content will be cached, separate to an AWS region/AZ
-> origin: origin of all files that CDN will distribute, this can be S3 bucket, EC2 instance, an Elastic Load Balancer or Route53
-> distribution: name given the CDN which consists of a collection of edge locations
-> users would just pull from local edge location that cached files retrieved already by different users, first user suffers performance penalty to access
-> can deliver entire website and other content using global network of edge locations, requests for your content are automatically routed to nearest edge location, so content is delivered with best possible performance
-> can work with any non-AWS origin server which stores the original, definitive versions of your files
-> web distribution: typically used for websites
-> RTMP: used for media streaming
-> edge location are not just read only, you can write to them too; objects cached for life of the TTL (Time To Live)
-> you can clear cached objects but you will be charged
- Create a CDN i.e. a distribution with origin in S3 bucket
-> go to CloudFront to create a distribution (set delivery method to either Web or RTMP streaming media files)
-> set origin domain name to be one of your S3 buckets/load balancer/EC2/web address of your on-prem origin and path, etc.
-> can set default cache behavior settings like HTTPS, requests, TTL (default for 24 hrs at edge location)
-> use presigned URLs/cookies to help with making sure you can view certain content with validated URLs after paying to secure objects in S3/CloudFront
-> can set distribution setings like web application firewall (WAF), SSL certificate, HTTP/2, etc.
-> takes a while to provision a distribution and to change things
-> can enable geo-restrictions (whitelist and blacklist certain countries), create invalidations to stop things from being cached but costs money, can add tags
- securing your buckets: by default all newly created buckets are private
-> can setup access control to your buckets using bucket policies, access control lists
-> S3 buckets can be configured to create access logs which log all requests made to the S3 bucket, can be done to another bucket
- encryption
-> in transit (when sending information to and from bucket) through SSL/TLS
-> at rest through server side encryption - S3 managed keys SSE-S3, AWS Key Management Service Managed Keys SSE-KMS, Server side encryption with customer provided keys SSE-C
and client side encryption
- Storage Gateway: service that connects an on-prem software appliance with cloud-based storage to provide seamless and secure integration between an organization's on-prem IT environment and AWS's storage infrastructure
and enables you to securely store data to AWS cloud for scalable and cost-effective storage
-> available for download as VM image to install on host in datacenter (VMware ESXi or Microsoft Hyper-V)
-> File Gateway (NFS): store flat files in S3, Volumes Gateway (iSCSI) - Stored Volumes (entire copy of dataset on-prem), Cached Volumes (only most frequently accessed data on-prem, rest on AWS)
and Tape Gateway (VTL): backup and archiving to eventually send to Glacier
-> File Gateway - files stored as objects in S3 buckets accessed through Network File System mount point, meta-data for permissions on S3 bucket
-> Volume Gateway - presents applications with disk volumes with iSCSI block protocol, can be asynchronously backed up as point-in-time snapshots and stored in cloud as Amazon EBS snapshots that only capture changed blocks and compressed
-> Stored Volumes - entire dataset is stored on site and is asynchronously backed up to S3
-> Cached Volumes - entire dataset stored on S3 and most frequently accessed data is cached on site
-> Gateway Virtual Tape Library (VTL) used for backup and uses popular backup applications like NetBackup, Backup Exec, Veam, etc. 
- AWS Import/Export Disk accelerates moving large amounts of data into and out of AWS cloud using portable storage devices for transport, uses Amazon's high-speed internal network
-> Snowball: petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS, 80TB snowball in all regions
-> Snowball Edge: 100TB data transfer device with on-board storage and compute capabilities
-> AWS Snowmobile: Exabyte-scale data transfer service to move extremely large amounts of data to AWS
-> Snowball can import to and export from S3
- S3 Transfer Acceleration: utilizes CloudFront Edge Network to accelerate uploads to S3, using a distinct URL to upload to edge location which will then transfer that file to S3 bucket
- S3 Review: object based to upload files, files from 0B to 5TB, unlimited storage, files stored in buckets, universal namespace (names must be unique globally)
-> Read after write consistency for PUTS of new objects, classes like S3, S3-IA, S3 Reduced Reduncancy Storage, Glacier
-> Key (name), value (data), version ID, metadata, access control lists, can store all versions of object (not for installing OS), backup tools, CloudFront CDN, bucket policies, encryption

Databases 101
- relational databases like traditional spreadsheet, tables, rows, fields (columns) - for online transaction processing
-> SQL Server, Oracle, MySQL server, PostgreSQL, Aurora, and MariaDB on RDS
- non relational databases like collections (table), document (row), key value pairs (fields) - JSON/NoSQL
-> like DynamoDB
- In-memory cache like ElastiCache - web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud
-> allowing you to retrieve information from fast, managed, in-memory caches instead of relying on slower disk-based databases
-> two open-source in-memory caching engines: Memcached, Redis
- Redshift for online analytics processing
- Database Migration Service (DMS) allows you to migrate production database to AWS

DynamoDB
- fast and flexible NoSQL database for apps that need consistent, single-digit millisecond latency at any scale that is fully managed database that supports both document and key-value data models
- stored on SSD storage and spread across 3 geographically distinct data centers
- eventual consistent reads (usually reached within a second and repeating a read after a short time should return updated data - default)
- tables, items (think of a row of data in table), attributes (think of column of data in a table)
- pricing: provisioned throughput capacity based on write and read throughput, first 25GB stored per month is free and then $0.25/GB per month
-> i.e. 1 million writes = 11.6 writes per second and 1 millions reads per day, 28GB of storage
and 1 DynamodDB write capacity unit can handle 1 write per second, so we would need 12 WCUs which charges $0.0065 for every 10 units ~ $0.1872 per day
and for 1 million writes = 11.6 writes per second and you would need 12 RCUs and charged $0.0065 for every 50 units ~$0.0374 per day
and since first 25GB free, we pay for 3GB per month ~$0.75 ==> $7.488 per month
-> free tier we get 25 RCU and 25 WCU
- i.e. creating a DynamoDB table, create a role for it in IAM and attach DynamoDB policies like full access, can go to DynamoDB console to check the table items
- two types of primary keys
-> single attribute (like uniqueID) - partition key (hash key) composed of one attribute
-> composite (like uniqueID and date range) - partition key and sort key (hash and range) composed of two attributes
- partition key: used as input to an internal hash function which determines the partition (physical location in which the data is stored), no two items in a table can have same partition key value
- partition key and sort key: two items can have same partition key but must have different sort key, all items with same partition key are stored together in sorted order by sort key value
- local secondary index: same partition key, different sort key; can only be created when creating table, cannot be removed or modified later
- global secondary index: has different partition key and different sort key, can be created at table creation or added later
- streams: used to capture any kind of modification to the DynamoDB tables
-> if new item added to table, stream captures image of entire item including all of its attributes
-> if item is updated, stream capture the "before" and "after" image of any attributes that were modified in the item
-> if an item is deleted from the table, the stream captures an image of the entire item before it was deleted
-> stored changes for up to 24hrs, can configure triggers Lambda functions to replicate this data in a table in another region or send an email with SES
-> i.e. triggering email send lambda function after signup, replicating data in response to new stream changes
- push button scalability, setting policies for tables
- query: finds items in a table using only primary key attribute values, must provide partition attribute name and distinct value to search for like id=17
-> can optionally provide a sort key attribute name and alue and use comparison operator to refine search results like user=17 and postedBy is past 7 days
-> by default query returns all of data attributes for items with specified primary keys but you can use ProjectionExpression parameter so that query only returns some of attributes rather than all of them
-> results sorted by sort key like in ascending numeric order or ASCII character code values, can set ScanIndexForward parameter to false to do descending order
-> by default eventually consistent but can be changed to strongly consistent (all writes before read will be captured)
- scan: examines every item in the table; by default returns all of the data attributes for every item but can use ProjectionExpression parameter to return only some of them
-> generally query operation is more efficient, scan always goes through entire table and filters out values to provide desired result (slow and can use up provisioned throughput for large table in single operation)
- for quicker response times, use Query, Get, or BatchGetItem APIs instead or use Scan operations in way that minimizes the impact on your table's request rate
- provisioned throughput calculations
-> unit of read provisioned throughput: all reads rounded up to increments of 4KB
-> eventually consistent reads by default consist of 2 reads per second
-> strongly consistent reads consist of 1 read per second
-> unit of write provisioned throughput: all writes are 1KB
-> all writes consist of 1 write per second
i.e. read 10 items of 1KB per second using eventual consistency. what should you set read throughput to?
(size of read rounded to nearest 4KB chunk / 4KB) * number of items = read throughput -> divide by 2 if eventually consistent (divide by 1 if strongly consistent)
1KB rounded to nearest 4KB increment = 4
4KB / 4KB = 1 read unit per item
1 * 10 read items = 10
eventual consistency is 10 / 2 = 5
5 units of read throughput
-> 400 HTTP Status Code - ProvisionedThroughputExceededException: exceeded max allowed provisioned throughput for table or for one or more global secondary indexes
- using web identity providers with DynamoDB: can use FB, Google, Amazon, Open-ID Connect-compatible Identity provider)
-> need to create a role first, need web identity token, app ID of provider, ARN of role
-> user authenticates with ID provider (such as Facebook), passed a token by ID provider, code calls AssumeRoleWithWebIdentity API and provides providers Token
and specifies ARN for the IAM role, app can now access DynamodDB from between 15 minutes to 1 hour
- conditional writes are idempotent, can send same conditional write request multiple times but have no further effect on item after first time DynamoDB performs specified update
- supports atomic counters where you use UpdateItem operation to increment/decrement value of existing attribute without interfering with other write requests (applied in order they are received)
-> not idempotent which means counter will increment each time you call UpdateItem, retrying could risk updating counter twice (banking app needs conditional update rather than atomic counter for like site visitor count)
- if you need to read multiple items you can use BatchGetItem API and a single BatchGetItem request can retrieve up to 1MB of data which can contain as many as 100 items and can retrieve from multiple tables

Notes for https://serverless-stack.com:
What is serverless?
- before charged with keeping server up even when not serving any HTTP requests, applying security updates,
scaling of resources
- serverless lets us build applications where we hand cloud provider like AWS our code and runs it for us;
we are charged for time it took for our code to execute and resources it consumed; runs in secured environment and may create
more instances to handle spikes in requests
- cloud provider handles requests and send us object with relevant info; requests treated as event and code is simply a function
that takes this as input
- AWS Lambda: write functions that repond to these events and when user makes a request, cloud provider creates a container and runs our
function inside it (i.e. two concurrent requests => two separate containers respond to requests)
// event provides information about event that triggered it like HTTP request, charged for every 100ms it uses and automatically
// scales to respond to usage; runtime comes with 512MB of ephemeral disk space and 3008MB of memory
exports.myHandler = function(event, context, callback) {
  // Do stuff
  // Error object and result object
  callback(Error error, Object result);
}
- serverless apps favored over traditional server hosted apps due to low maintenance and cost and easy to scale
-> paying per request and only concerned about your code
1. create AWS account

2. create an IAM user (Identity and Access Management) - enables you to manage users and user permissions in AWS
i.e. need to create one or more users for someone who needs access to console or when new app needs to make API calls to AWS
- we create one account for AWS CLI and serverless framework to connect to AWS API directly and not use management console
- IAM - helps with security control access for AWS services, controlling who can use your AWS resources (authentication) and what resources
they can use and in what ways (authorization)
-> you are root user when you first create AWS account; IAM user for when you are not the only person working in your account and you don't want
to give root credentials
-> IAM user consists of name, password to sign into AWS Management Console and up to two access keys that can be used with API or CLI
-> by default users can't access anything in your account but you grant permissions to user by creating policy and attaching policy to user to 
restrict what they can and cannot access
- IAM policy: rule or set of rules defining the operations allowed/denied to be performed on AWS resource
i.e. managed policy (pre-defined ones like AmazonS3ReadOnlyAccess)
inline policy = custom polciy created by hand
adding user to group that has appropriate permission policies attached
cloning permission of existing IAM user
i.e. policy granting all operations to all S3 buckets
{
  "Version": "2012-10-17",
  "Statement": {
    "Effect": "Allow",
    "Action": "s3:*",
    "Resource": "*"
  }
}
-> policies depend on the resource ARN for Resource property: identifier for a resource in AWS;
service actions - Action and condition context keys - Condition properties
- sometimes AWS resources need access to other resources in account like Lambda function that queries DynamoDB to retrieve
some data (and Lambda function should only make read queries)- we need IAM role to be secure: identity with permission policies
that determine what the identity can and cannot do in AWS; doesn't have any credentials (password or access keys) associated with it
-> IAM role can be taken by anyone who needs it so Lambda function assigned role to temporarily take on the permission; can be 
applied to users as well and can be re-used to anybody else; can also have role tied to the ARN of user from different organization to
allow the external user to assume the role as part of your organization like third party service acting on your AWS organization
(Cross-Account IAM Role and add external as Trust Relationship)
- IAM Group: collection of IAM users, specify permissions for collection of users i.e. admins group
- ARN (Amazon Resource Names) uniquely identify AWS resources
-> formats like this:
arn:partition:service:region:account-id:resource
arn:partition:service:region:account-id:resourcetype/resource
arn:partition:service:region:account-id:resourcetype:resource
i.e.
<!-- IAM user name -->
arn:aws:iam::123456789012:user/David
<!-- Amazon RDS instance used for tagging -->
arn:aws:rds:eu-west-1:123456789012:db:mysql-db
<!-- Object in an Amazon S3 bucket -->
arn:aws:s3:::my_corporate_bucket/exampleobject.png
-> used for communication so you can reference specific resource when orchestrating a system that involves multiple AWS
resources like API Gateway listening for RESTful APIs and invoking corresponding Lambda function based on API path and request method
i.e. GET /hello_world => arn:aws:lambda:us-east-1:123456789012:function:lambda-hello-world
-> used in IAM policy to define which resource like S3 bucket the access is granted for
i.e.
{
  "Version": "2012-10-17",
  "Statement": {
    "Effect": "Allow",
    "Action": ["s3:GetObject"],
    "Resource": "arn:aws:s3:::Hello-bucket/*"
}

3. Configure AWS CLI
- need pip and python -> do aws configure to set up with your Access Key ID and Secret Access Key from IAM user

4. Creating a DynamoDB table
- fully managed NoSQL database with great performance and scalability, each table contains multiple items and each
item composed of one or more attributes
- each table with primary key which cannot be changed once set
-> primary key uniquely identifies each item in the table so no two items can have same key
-> can have partition key or partition key and sort key (composite)
-> use composite for better querying
-> can select DynamoDD AutoScaling Service Linked Role
-> default settings provisions 5 reads and writes but you can specify how much provisioned throughput capacity you want to reserve
for reads and writes i.e. 1 RCU can read up to 8KB per second and one WCU can write up to 1KB per second

5. Creating S3 Bucket for file uploads/attachments
- S3: storage service through web services interfaces like REST
-> can store any object on S3 like images, videos, files, etc.; objects organized into buckets and identified within each
bucket by a unique, user-assigned key
-> pick name and select region; bucket names are globally unique and region = physical geographical region where files are stored
- since app will be served through custom domain and thus communicating across domains for uploads, S3 does not allow its resources to be accessed from a
different domain; however cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to
interact with resources in different domain -> should enable CORS for S3 bucket
i.e. should alter the AllowedOrigin to use your domain or list of domains when in production
<CORSConfiguration>
	<CORSRule>
		<AllowedOrigin>*</AllowedOrigin>
		<AllowedMethod>GET</AllowedMethod>
		<AllowedMethod>PUT</AllowedMethod>
		<AllowedMethod>POST</AllowedMethod>
		<AllowedMethod>HEAD</AllowedMethod>
		<MaxAgeSeconds>3000</MaxAgeSeconds>
		<AllowedHeader>*</AllowedHeader>
	</CORSRule>
</CORSConfiguration>

6. Create a Cognito User Pool
- to handle user accounts and authentication with Amazon Cognito
- Amazon Cognito User Pool to make it easy for developers to add sign-up and sign-in functionality to web and mobile
applications; serves as your own identity provider to maintain user directory; supports user registration and sign-in and
provisioning identity tokens for signed-in users
- can make users sign up and login with email as username/username
-> can set required attributes upon signup such as username, email, name; won't be able to change these requirements after pool is created
-> need to set up App client (not with client secret because not supported by JS SDK), must enable sign-in API for server-based auth as it
is required by AWS CLI when managing pool users via command line interface (to test creating user through command line interface)
- create domain name as it will be used for sign-up and sign-in pages hosted by Cognito
- Cognito User Pool will maintain a user directory for app and used to authenticate access to our API
- can create a Cognito test user
i.e. Signing up
aws cognito-idp sign-up
  --region <your-app-region>
  --client-id <app-client-id>
  --username <some-email>
  --password <some-password>
  --user-attributes Name=name,Value=Alfred Name=preferred_username,Value=alfredlucero
i.e. Verifying account
aws cognito-idp admin-confirm-sign-up
  --region <your-app-region>
  --user-pool-id <user-pool-id>
  --username <some-email>

7. Setting up Serverless framework
- going to use AWS Lambda and Amazon API Gateway to create backend
-> AWS Lambda = compute service that lets you run code without provisioning or managing servers and pay for compute time
-> API Gateway = create, publish, maintain, monitor, and secure APIs
-> can use Serverless framework to help configuring Lambda and Gateway; can deploy backend applications as independent functions
that will be deployed to AWS Lambda and configures AWS Lambda to run code in response to HTTP requests using Amazon API Gateway
- npm install serverless -g; serverless create --template aws-nodejs (creates an AWS Node.js service)
-> creates handler.js (contains actual code for services/functions deployed to AWS Lambda) and serverless.yml (configuration on what AWS services Serverless will provision and how to configure them)
- npm install aws-sdk --save-dev; to talk to AWS services
- npm install uuid --save; to generate unique ids to store things in DynamoDB
- by default AWS Lambda only supports specific of JS and not up to date Node.js engine
-> we can enable ES6/7 by setting up Babel and Webpack to transpile and package our project
-> need babel-core, babel-loader, babel-plugin-transform-runtime, babel-preset-env, babel-preset-stage-3, serverless-webpack (to trigger Webpack build when we run Serverless commands),
webpack-node-externals (because we do not want Webpack to bundle our aws-sdk module since not compatible)

8. Creating the API
- can set region while connecting to DynamoDB
- parse input from event.body that represents the HTTP request parameters
- userId is a federated identity id that comes as part of request, set after user authenticated via User Pool
- can create ids with uuid and set dateCreated; enable CORS; must update serverless.yml functions
- can create mocks to mimic requests in a .json file and run things like:
serverless invoke local --function create --path mocks/create-event.json
-> if with multiple profiles for AWS SDK credentials can pick like (can configure multiple AWS profiles in Serverless):
AWS_PROFILE=myProfile serverless invoke local --function create --path mocks/create-event.json